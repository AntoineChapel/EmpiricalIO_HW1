{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "from IPython.display import display, Latex\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.71]\n",
      " [ 0.44]\n",
      " [-1.37]\n",
      " [-0.91]\n",
      " [-1.23]\n",
      " [ 1.  ]] \n",
      "\n",
      "[[3.22 0.   0.   0.   0.   0.  ]\n",
      " [0.   3.24 0.   0.   0.   0.  ]\n",
      " [0.   0.   2.87 0.   0.   0.  ]\n",
      " [0.   0.   0.   4.15 0.   0.  ]\n",
      " [0.   0.   0.   0.   1.38 0.  ]\n",
      " [0.   0.   0.   0.   0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "N = 1000\n",
    "J = 4\n",
    "T = 150\n",
    "\n",
    "# Generate the data\n",
    "np.random.seed(123)\n",
    "mu = np.array([-1.71, 0.44, -1.37, -0.91, -1.23, 1]).reshape(-1, 1)\n",
    "sigma = np.diag(np.array([3.22, 3.24, 2.87, 4.15, 1.38, 1])).reshape(6, 6)\n",
    "\n",
    "#sigma = np.diag(np.zeros(6)).reshape(6, 6) #This was run as a test for theta MLE homogeneous\n",
    "# Under the \"correct\" distributional assumption, that is if we generate homogeneous consumers,\n",
    "# our MLE hmg estimate for Theta, including gamma, is very well identified. So any problem in \n",
    "# identification encountered later is driven by consumer heterogeneity as a confounder.\n",
    "\n",
    "print(mu, '\\n')\n",
    "print(sigma)\n",
    "\n",
    "\n",
    "# generate the random parameters\n",
    "betas = np.random.multivariate_normal(mu.flatten(), sigma, N)\n",
    "betas_np = betas[:, :-2]\n",
    "etas_np = betas[:, -2]\n",
    "gammas_np = betas[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_transition_states = pd.read_csv(r'price_transition_states.csv')\n",
    "price_transition_matrix = pd.read_csv(r'transition_prob_matrix.csv')\n",
    "price_transition_matrix_np = price_transition_matrix.to_numpy()\n",
    "price_transition_states_np = price_transition_states.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_prices(states, transition, T):\n",
    "    state_indices = np.arange(states.shape[0])\n",
    "\n",
    "    price_simu = np.zeros((T, 6)) #create a matrix to store the simulated prices\n",
    "    price_simu[0] = states[0] #fix the initial vector of prices\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        preceding_state = price_simu[t-1, :] #take the preceding state\n",
    "        index_preceding_state = int(preceding_state[-1] - 1) #take the index of the preceding state (-1 for 0-indexing in Python)\n",
    "        index_next_state = np.random.choice(state_indices, p=(transition[index_preceding_state, :].flatten())) #draw the next state\n",
    "        price_simu[t, :] = states[index_next_state] #update the price vector and store it\n",
    "    return price_simu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_150_by_6 = simulate_prices(price_transition_states_np, price_transition_matrix_np, T)\n",
    "prices_150_by_4 = price_150_by_6[:, :-2] #remove the indices column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate baseline utility data (no loyalty)\n",
    "utility_np = np.zeros((T, 1+J, N)) # 1 for the outside option, J for the number of products\n",
    "for t in range(1, T):\n",
    "    for i in range(N):\n",
    "        utility_np[t, 0, i] = np.random.gumbel() #outside option, just a random noise\n",
    "        utility_np[t, 1:, i] = betas_np[i, :] + etas_np[i]*prices_150_by_4[t, :] + np.random.gumbel(size=J) #utility for the J products\n",
    "\n",
    "#utility_np_orig = utility_np.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add loyalty\n",
    "state_matrix = np.zeros((T, N), dtype=int) #the state at time 0 is 0\n",
    "state_matrix[1, :] = np.argmax(utility_np[0, :, :], axis=0) #initialize the state simulation\n",
    "\n",
    "for t in range(1, T-1):\n",
    "    for i in range(N):\n",
    "        state_it = state_matrix[t, i]\n",
    "        for j in range(1, J+1): #exclude the outside option\n",
    "            utility_np[t, j, i] += gammas_np[i] * (j == state_it)\n",
    "        choice = np.argmax(utility_np[t, :, i])\n",
    "        if choice==0:\n",
    "            state_matrix[t+1, i] = state_it ### if the outside option is chosen, the state remains the same\n",
    "        else:\n",
    "            state_matrix[t+1, i] = choice ### if a product is chosen, the state is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility_orig_jnp = jnp.array(utility_np_orig[100:, :, :])  #50 x 5 x 1000\n",
    "utility_jnp = jnp.array(utility_np[100:, :, :])            #50 x 5 x 1000\n",
    "choice_jnp = jnp.argmax(utility_np, axis=1)[100:, :]       #50 x 1000\n",
    "prices_50_by_4_jnp = jnp.array(prices_150_by_4[100:, :])   #50 x 4\n",
    "state_matrix_jnp = jnp.array(state_matrix[100:, :])        #50 x 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i: Homogeneous, Two classes, and Mixed Logit\n",
    "* Homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def ccp(theta):\n",
    "    \"\"\"\n",
    "    Compute the choice probabilities for each time period and product for a given theta, for each possible state\n",
    "    There are 4 possible states (individuals are never in state 0). For a given theta, compute the choice probabilities for each state\n",
    "    Should a return a (T, J, J+1) array. That is, for each period, for each possible state, the choice probas\n",
    "    \"\"\"\n",
    "    theta_jnp = jnp.array(theta).flatten()\n",
    "    betas = theta_jnp[:-2]\n",
    "    eta = theta_jnp[-2]\n",
    "    gamma = theta_jnp[-1]\n",
    "    \n",
    "    #possible states: 0, 1, 2, 3, 4 \n",
    "    v_1to4_utility_state0 = (betas + eta * prices_50_by_4_jnp).reshape(50, 1, 4)\n",
    "    v_1to4_utility_state1to4 = (betas + eta * prices_50_by_4_jnp).reshape(50, 1, 4) + gamma * jnp.eye(4)\n",
    "    v_utility = jnp.concatenate((v_1to4_utility_state0, v_1to4_utility_state1to4), axis=1)\n",
    "    v_default = jnp.zeros((50, 5, 1))\n",
    "    v_utility_full = jnp.concatenate((v_default, v_utility), axis=2)\n",
    "\n",
    "    # Compute choice probabilities \n",
    "    log_sumexps = logsumexp(v_utility_full, axis=2, keepdims=True)\n",
    "    probas = jnp.exp(v_utility_full - log_sumexps) #get the choice probabilities for each time period and product\n",
    "\n",
    "    return probas\n",
    "ccp_vec = vmap(ccp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood(theta): #(log)-likelihood function\n",
    "    probas_theta = ccp(theta) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "grad_likelihood = jit(grad(likelihood)) ## gradient of the likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_adam(f, grad_f, x0, norm=1e9, tol=0.1, lr=0.05, maxiter=1000, verbose=0, *args): ## generic adam optimizer\n",
    "  \"\"\"\n",
    "  Generic Adam Optimizer. Specify a function f, a starting point x0, possibly a \\n\n",
    "  learning rate in (0, 1). The lower the learning rate, the more stable (and slow) the convergence.\n",
    "  \"\"\"\n",
    "  tic = time.time()\n",
    "  solver = optax.adam(learning_rate=lr)\n",
    "  params = jnp.array(x0, dtype=jnp.float32)\n",
    "  opt_state = solver.init(params)\n",
    "  iternum = 0\n",
    "  while norm > tol and iternum < maxiter :\n",
    "    iternum += 1\n",
    "    grad = grad_f(params, *args)\n",
    "    updates, opt_state = solver.update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    params = jnp.asarray(params, dtype=jnp.float32)\n",
    "    norm = jnp.max(jnp.abs(grad))\n",
    "    if verbose > 0:\n",
    "      if iternum % 100 == 0:\n",
    "        print(f\"Iteration: {iternum}  Norm: {norm}  theta: {jnp.round(params, 2)}\")\n",
    "    if verbose > 1:\n",
    "      print(f\"Iteration: {iternum}  Norm: {norm}  theta: {jnp.round(params, 2)}\")\n",
    "  tac = time.time()\n",
    "  if iternum == maxiter:\n",
    "    print(f\"Convergence not reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "  else:\n",
    "    print(f\"Convergence reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 7784.0830078125  theta: [-0.84999996  0.31       -0.82       -0.25       -0.55        0.93      ]\n",
      "Iteration: 200  Norm: 3878.31787109375  theta: [-1.31        0.19999999 -1.24       -0.14       -0.68        1.5999999 ]\n",
      "Iteration: 300  Norm: 1851.5341796875  theta: [-1.62  0.08 -1.53 -0.16 -0.75  2.  ]\n",
      "Iteration: 400  Norm: 887.0517578125  theta: [-1.8299999 -0.06      -1.74      -0.25      -0.76       2.23     ]\n",
      "Iteration: 500  Norm: 418.6636657714844  theta: [-1.99 -0.21 -1.88 -0.37 -0.74  2.36]\n",
      "Iteration: 600  Norm: 192.72698974609375  theta: [-2.1        -0.35999998 -1.99       -0.52       -0.7         2.4199998 ]\n",
      "Iteration: 700  Norm: 137.88356018066406  theta: [-2.19 -0.51 -2.07 -0.68 -0.65  2.46]\n",
      "Iteration: 800  Norm: 120.94245910644531  theta: [-2.26       -0.65       -2.1299999  -0.83       -0.59999996  2.48      ]\n",
      "Iteration: 900  Norm: 100.07978820800781  theta: [-2.31 -0.77 -2.19 -0.96 -0.55  2.49]\n",
      "Iteration: 1000  Norm: 79.70939636230469  theta: [-2.36      -0.88      -2.23      -1.0799999 -0.51       2.49     ]\n",
      "Iteration: 1100  Norm: 61.69140625  theta: [-2.3999999  -0.96999997 -2.27       -1.18       -0.48        2.5       ]\n",
      "Iteration: 1200  Norm: 46.59613037109375  theta: [-2.44 -1.05 -2.3  -1.26 -0.45  2.5 ]\n",
      "Iteration: 1300  Norm: 34.40406799316406  theta: [-2.46      -1.11      -2.32      -1.3199999 -0.42       2.5      ]\n",
      "Iteration: 1400  Norm: 24.843597412109375  theta: [-2.48 -1.16 -2.34 -1.38 -0.41  2.5 ]\n",
      "Iteration: 1500  Norm: 17.533493041992188  theta: [-2.5       -1.1899999 -2.36      -1.42      -0.39       2.51     ]\n",
      "Iteration: 1600  Norm: 12.084869384765625  theta: [-2.51      -1.22      -2.37      -1.4499999 -0.38       2.51     ]\n",
      "Iteration: 1700  Norm: 8.123641967773438  theta: [-2.52      -1.24      -2.37      -1.4699999 -0.37       2.51     ]\n",
      "Iteration: 1800  Norm: 5.3233489990234375  theta: [-2.52      -1.25      -2.3799999 -1.48      -0.37       2.51     ]\n",
      "Iteration: 1900  Norm: 3.3855743408203125  theta: [-2.53       -1.26       -2.3799999  -1.49       -0.35999998  2.51      ]\n",
      "Iteration: 2000  Norm: 2.0966796875  theta: [-2.53       -1.27       -2.3899999  -1.5        -0.35999998  2.51      ]\n",
      "Iteration: 2100  Norm: 1.2570953369140625  theta: [-2.53       -1.27       -2.3899999  -1.51       -0.35999998  2.51      ]\n",
      "Iteration: 2200  Norm: 0.7418975830078125  theta: [-2.53       -1.28       -2.3899999  -1.51       -0.35999998  2.51      ]\n",
      "Iteration: 2300  Norm: 0.42067718505859375  theta: [-2.53       -1.28       -2.3899999  -1.51       -0.35999998  2.51      ]\n",
      "Iteration: 2400  Norm: 0.222076416015625  theta: [-2.54       -1.28       -2.3899999  -1.51       -0.35999998  2.51      ]\n",
      "Iteration: 2500  Norm: 0.12180328369140625  theta: [-2.54       -1.28       -2.3899999  -1.51       -0.35999998  2.51      ]\n",
      "Convergence reached after 2517 iterations. \n",
      "Time: 29.785691499710083 seconds. Norm: 0.09705352783203125\n"
     ]
    }
   ],
   "source": [
    "theta_MLE_homo = minimize_adam(likelihood, grad_likelihood, jnp.zeros(6), lr=0.01, verbose=1, maxiter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5353696  -1.2799662  -2.3902292  -1.5117126  -0.35722518  2.5106406 ]\n"
     ]
    }
   ],
   "source": [
    "print(theta_MLE_homo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computation of standard errors\n",
    "\n",
    "@jit\n",
    "def likelihood_it(theta, i, t):\n",
    "    \"\"\"\n",
    "    Computes the likelihood for an individual observation\n",
    "    \"\"\"\n",
    "    probas_theta = ccp(theta)\n",
    "    likelihood_it = jnp.log(probas_theta[t, state_matrix_jnp[t, i], choice_jnp[t, i]])\n",
    "    return likelihood_it\n",
    "\n",
    "grad_likelihood_it = jit(grad(likelihood_it)) ### Takes the gradient of the individual likelihood\n",
    "\n",
    "@jit\n",
    "def outer_grad_likelihood(theta, i, t):\n",
    "    \"\"\"\n",
    "    Takes the outer product (column vector x row vector) of the gradient of the individual likelihood\n",
    "    \"\"\"\n",
    "    grad_it = (grad_likelihood_it(theta, i, t)).reshape(-1, 1) \n",
    "    return grad_it@grad_it.T\n",
    "\n",
    "\n",
    "#computes the outer product above for each individual and time period\n",
    "grad_likelihood_it_vec = vmap(vmap(outer_grad_likelihood, in_axes=(None, 0, None)), in_axes=(None, None, 0)) \n",
    "\n",
    "@jit\n",
    "def compute_standard_errors(theta):\n",
    "    sum_outers = jnp.sum(grad_likelihood_it_vec(theta, jnp.arange(N), jnp.arange(T)), axis=(0, 1))\n",
    "    return jnp.diag(jnp.sqrt(jnp.linalg.inv(sum_outers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.02599621, 0.05506609, 0.02310293, 0.06025254, 0.02323857,\n",
       "       0.00804735], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se = compute_standard_errors(theta_MLE_homo)\n",
    "se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLE 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.6133583 -1.4451644 -2.459538  -1.6924702 -0.4269409  2.4864986]\n",
      "[-2.457381   -1.114768   -2.3209205  -1.3309549  -0.28750947  2.5347826 ]\n"
     ]
    }
   ],
   "source": [
    "###Two classes: instead of estimating theta, we want to estimate the weights phi_1, phi_2 of each class (?)\n",
    "theta_k1 = theta_MLE_homo - 3*se\n",
    "theta_k2 = theta_MLE_homo + 3*se\n",
    "print(theta_k1)\n",
    "print(theta_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def choice_probas_2classes(phi1):\n",
    "    phi2 = 1 - phi1\n",
    "    probas_k1 = ccp(theta_k1)\n",
    "    probas_k2 = ccp(theta_k2)\n",
    "    probas = phi1 * probas_k1 + phi2 * probas_k2\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood_2classes(phi1): #(log)-likelihood function\n",
    "    probas_theta = choice_probas_2classes(phi1) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "\n",
    "grad_likelihood_2classes = jit(grad(likelihood_2classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 71 iterations. \n",
      "Time: 1.3041553497314453 seconds. Norm: 0.09765625\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_1^h$: [-2.6133583 -1.4451644 -2.459538  -1.6924702 -0.4269409  2.4864986]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_2^h$: [-2.457381   -1.114768   -2.3209205  -1.3309549  -0.28750947  2.5347826 ]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (0.5020579099655151, 0.49794209003448486)\n"
     ]
    }
   ],
   "source": [
    "weight_1 = minimize_adam(likelihood_2classes, grad_likelihood_2classes, 0.5, verbose=False)\n",
    "display(Latex(f'$\\Theta_1^h$: {theta_k1}'))\n",
    "display(Latex(f'$\\Theta_2^h$: {theta_k2}'))\n",
    "print(f'Weights: {weight_1.item(), 1-weight_1.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLE assuming that $\\Theta^h$ has a normal distribution across the households "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: sanity check. Let's assume we know $\\sigma$: do we recover the proper mu under the correct distributional assumption ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of Monte Carlo draws\n",
    "S = 1000  # Number of simulation draws\n",
    "\n",
    "# Generate random draws for Monte Carlo integration\n",
    "key = jax.random.PRNGKey(123)\n",
    "mc_draws = jax.random.normal(key, (S, 6))  # 5 parameters (4 betas + 1 eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood_correct(theta):\n",
    "    mu = theta.flatten()  # Mean of the random parameters\n",
    "    betas_eta = mu + mc_draws * jnp.sqrt(jnp.diag(sigma))\n",
    "    \n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood_correct = jit(grad(mixed_logit_likelihood_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 587 iterations. \n",
      "Time: 15.25252914428711 seconds. Norm: 0.0977182388305664\n",
      "[-3.1571164 -0.8895682 -2.804269  -1.686722  -1.1715082  3.8051717]\n"
     ]
    }
   ],
   "source": [
    "### This does not converge properly and as the norm reduces, the bias is still very high\n",
    "theta_mixed_logit_correct = minimize_adam(mixed_logit_likelihood_correct, \n",
    "                                          grad_mixed_logit_likelihood_correct, \n",
    "                                          theta_MLE_homo, \n",
    "                                          lr=0.05, maxiter=5000, verbose=0, tol=0.1)\n",
    "print(theta_mixed_logit_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is bias. Maybe this is to be expected given the instructions of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood(theta):\n",
    "    mu = theta[:6]  # Mean of the random parameters\n",
    "    sigma = jnp.exp(theta[6:])\n",
    "    betas_eta = mu + mc_draws * sigma\n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood = jit(grad(mixed_logit_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 21.240543365478516  theta: [-4.47       -0.98999995 -3.98       -1.36       -0.55        3.1\n",
      "  0.96       -2.1699998   1.         -2.22       -2.8799999   1.1999999 ]\n",
      "Iteration: 200  Norm: 0.8320649266242981  theta: [-4.69       -0.96999997 -4.14       -1.36       -0.55        3.1399999\n",
      "  1.02       -2.6499999   1.05       -2.45       -2.98        1.3       ]\n",
      "Iteration: 300  Norm: 0.579755961894989  theta: [-4.77       -0.96999997 -4.2        -1.37       -0.55        3.1599998\n",
      "  1.04       -3.1399999   1.06       -2.6        -3.04        1.3399999 ]\n",
      "Iteration: 400  Norm: 0.4046667218208313  theta: [-4.79       -0.96999997 -4.21       -1.37       -0.55        3.1599998\n",
      "  1.05       -3.55        1.0699999  -2.72       -3.08        1.35      ]\n",
      "Iteration: 500  Norm: 0.295127809047699  theta: [-4.79       -0.96999997 -4.21       -1.37       -0.55        3.1599998\n",
      "  1.05       -3.8999999   1.0699999  -2.82       -3.12        1.3399999 ]\n",
      "Iteration: 600  Norm: 0.22442765533924103  theta: [-4.7799997  -0.96999997 -4.21       -1.37       -0.55        3.1599998\n",
      "  1.04       -4.19        1.0699999  -2.8999999  -3.1499999   1.3399999 ]\n",
      "Convergence reached after 647 iterations. \n",
      "Time: 16.74734616279602 seconds. Norm: 0.19973911345005035\n"
     ]
    }
   ],
   "source": [
    "### This does not seem to converge well and the bias is high.\n",
    "x_start = jnp.concatenate((theta_MLE_homo, jnp.zeros(6)))\n",
    "theta_mixed_logit = minimize_adam(mixed_logit_likelihood, grad_mixed_logit_likelihood, x_start, lr=0.3, maxiter=1000, verbose=1, tol=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-4.7796974 , -0.96995264, -4.20472   , -1.366045  , -0.5476076 ,\n",
       "        3.1601624 ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_mixed_logit[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1.6847361 , 0.11596691, 1.7028682 , 0.22995608, 0.20615704,\n",
       "       1.9556462 ], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.sqrt(jnp.exp(theta_mixed_logit[6:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii: Re-initialize the initial state and re-run the state computation given choice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_matrix_np2 = np.array(jnp.vstack((jnp.zeros((1, 1000)), state_matrix_jnp[1:, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###rerun the state based on choices. Takes 9 secs.\n",
    "for i in range(N):\n",
    "    for t in range(49):\n",
    "        choice_it = choice_jnp[t, i]\n",
    "        if choice_it != 0:\n",
    "            state_matrix_np2[t+1, i] = choice_it\n",
    "        else:\n",
    "            state_matrix_np2[t+1, i] = state_matrix_np2[t, i]\n",
    "state_matrix_jnp2 = jnp.array(state_matrix_np2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, sanity check. Let's assume sigma is known, and see if we can recover the true mu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood_correct2(theta):\n",
    "    mu = theta.flatten()  # Mean of the random parameters\n",
    "    betas_eta = mu + mc_draws * jnp.sqrt(jnp.diag(sigma))\n",
    "    \n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp2, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood_correct2 = jit(grad(mixed_logit_likelihood_correct2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 620 iterations. \n",
      "Time: 15.92100715637207 seconds. Norm: 0.09795951843261719\n",
      "[-2.8360603  -0.38305134 -2.5122223  -1.1973017  -1.3416026   3.8803911 ]\n"
     ]
    }
   ],
   "source": [
    "### Much closer, but still not there, and one sign is off.\n",
    "theta_mixed_logit_correct2 = minimize_adam(mixed_logit_likelihood_correct2, \n",
    "                                          grad_mixed_logit_likelihood_correct2, \n",
    "                                          theta_MLE_homo, \n",
    "                                          lr=0.05, maxiter=5000, verbose=0, tol=0.1)\n",
    "print(theta_mixed_logit_correct2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Mixed Logit estimation of $\\Theta$ and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood2(theta):\n",
    "    mu = theta[:6]  # Mean of the random parameters\n",
    "    sigma = jnp.exp(theta[6:]) \n",
    "    betas_eta = mu + mc_draws * sigma\n",
    "    \n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    \n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp2, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood2 = jit(grad(mixed_logit_likelihood2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 17.723278045654297  theta: [-4.17       -0.47       -3.75       -0.85999995 -0.72999996  3.27\n",
      "  0.95       -2.1399999   1.         -1.9        -2.6299999   1.22      ]\n",
      "Iteration: 200  Norm: 0.8716120719909668  theta: [-4.14       -0.47       -3.7099998  -0.87       -0.72999996  3.25\n",
      "  0.95       -2.72        0.98999995 -2.09       -2.75        1.1999999 ]\n",
      "Iteration: 300  Norm: 0.5742689967155457  theta: [-4.12       -0.47       -3.6999998  -0.87       -0.71999997  3.25\n",
      "  0.94       -3.23        0.98999995 -2.25       -2.85        1.1899999 ]\n",
      "Iteration: 400  Norm: 0.39445576071739197  theta: [-4.11       -0.48       -3.6899998  -0.87       -0.71999997  3.24\n",
      "  0.94       -3.6599998   0.97999996 -2.36       -2.9199998   1.18      ]\n",
      "Iteration: 500  Norm: 0.2860749661922455  theta: [-4.1        -0.48       -3.6799998  -0.87       -0.71999997  3.24\n",
      "  0.93       -4.          0.97999996 -2.46       -2.98        1.18      ]\n",
      "Iteration: 600  Norm: 0.21706745028495789  theta: [-4.0899997  -0.48       -3.6799998  -0.87       -0.71999997  3.23\n",
      "  0.93       -4.29        0.97999996 -2.53       -3.02        1.17      ]\n",
      "Convergence reached after 633 iterations. \n",
      "Time: 16.193182468414307 seconds. Norm: 0.19978854060173035\n"
     ]
    }
   ],
   "source": [
    "### This does not converge easily. Under more cautious learning rates, the bias is very high\n",
    "\n",
    "x_start = jnp.concatenate((theta_MLE_homo, jnp.zeros(6)))\n",
    "theta_mixed_logit2 = minimize_adam(mixed_logit_likelihood2, grad_mixed_logit_likelihood2, x_start, lr=0.3, maxiter=1000, verbose=1, tol=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance-Covariance Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1.6847361 , 0.11596691, 1.7028682 , 0.22995608, 0.20615704,\n",
       "       1.9556462 ], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.sqrt(jnp.exp(theta_mixed_logit[6:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EIO_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

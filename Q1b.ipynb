{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "from IPython.display import display, Latex\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.71]\n",
      " [ 0.44]\n",
      " [-1.37]\n",
      " [-0.91]\n",
      " [-1.23]\n",
      " [ 1.  ]] \n",
      "\n",
      "[[3.22 0.   0.   0.   0.   0.  ]\n",
      " [0.   3.24 0.   0.   0.   0.  ]\n",
      " [0.   0.   2.87 0.   0.   0.  ]\n",
      " [0.   0.   0.   4.15 0.   0.  ]\n",
      " [0.   0.   0.   0.   1.38 0.  ]\n",
      " [0.   0.   0.   0.   0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "N = 1000\n",
    "J = 4\n",
    "T = 150\n",
    "\n",
    "# Generate the data\n",
    "np.random.seed(123)\n",
    "mu = np.array([-1.71, 0.44, -1.37, -0.91, -1.23, 1]).reshape(-1, 1)\n",
    "sigma = np.diag(np.array([3.22, 3.24, 2.87, 4.15, 1.38, 1])).reshape(6, 6)\n",
    "\n",
    "print(mu, '\\n')\n",
    "print(sigma)\n",
    "\n",
    "\n",
    "# generate the random parameters\n",
    "betas = np.random.multivariate_normal(mu.flatten(), sigma, N)\n",
    "betas_np = betas[:, :-2]\n",
    "etas_np = betas[:, -2]\n",
    "gammas_np = betas[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_transition_states = pd.read_csv(r'price_transition_states.csv')\n",
    "price_transition_matrix = pd.read_csv(r'transition_prob_matrix.csv')\n",
    "price_transition_matrix_np = price_transition_matrix.to_numpy()\n",
    "price_transition_states_np = price_transition_states.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_prices(states, transition, T):\n",
    "    state_indices = np.arange(states.shape[0])\n",
    "\n",
    "    price_simu = np.zeros((T, 6)) #create a matrix to store the simulated prices\n",
    "    price_simu[0] = states[0] #fix the initial vector of prices\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        preceding_state = price_simu[t-1, :] #take the preceding state\n",
    "        index_preceding_state = int(preceding_state[-1] - 1) #take the index of the preceding state (-1 for 0-indexing in Python)\n",
    "        index_next_state = np.random.choice(state_indices, p=(transition[index_preceding_state, :].flatten())) #draw the next state\n",
    "        price_simu[t, :] = states[index_next_state] #update the price vector and store it\n",
    "    return price_simu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_150_by_6 = simulate_prices(price_transition_states_np, price_transition_matrix_np, T)\n",
    "prices_150_by_4 = price_150_by_6[:, :-2] #remove the indices column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate baseline utility data (no loyalty)\n",
    "utility_np = np.zeros((T, 1+J, N)) # 1 for the outside option, J for the number of products\n",
    "for t in range(1, T):\n",
    "    for i in range(N):\n",
    "        utility_np[t, 0, i] = np.random.gumbel() #outside option, just a random noise\n",
    "        utility_np[t, 1:, i] = betas_np[i, :] + etas_np[i]*prices_150_by_4[t, :] + np.random.gumbel(size=J) #utility for the J products\n",
    "\n",
    "#utility_np_orig = utility_np.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add loyalty\n",
    "state_matrix = np.zeros((T, N), dtype=int) #the state at time 0 is 0\n",
    "state_matrix[1, :] = np.argmax(utility_np[0, :, :], axis=0) #initialize the state simulation\n",
    "\n",
    "for t in range(1, T-1):\n",
    "    for i in range(N):\n",
    "        state_it = state_matrix[t, i]\n",
    "        for j in range(1, J+1): #exclude the outside option\n",
    "            utility_np[t, j, i] += gammas_np[i] * (j == state_it)\n",
    "        choice = np.argmax(utility_np[t, :, i])\n",
    "        if choice==0:\n",
    "            state_matrix[t+1, i] = state_it ### if the outside option is chosen, the state remains the same\n",
    "        else:\n",
    "            state_matrix[t+1, i] = choice ### if a product is chosen, the state is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility_orig_jnp = jnp.array(utility_np_orig[100:, :, :])  #50 x 5 x 1000\n",
    "utility_jnp = jnp.array(utility_np[100:, :, :])            #50 x 5 x 1000\n",
    "choice_jnp = jnp.argmax(utility_np, axis=1)[100:, :]       #50 x 1000\n",
    "prices_50_by_4_jnp = jnp.array(prices_150_by_4[100:, :])   #50 x 4\n",
    "state_matrix_jnp = jnp.array(state_matrix[100:, :])        #50 x 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def ccp(theta):\n",
    "    \"\"\"\n",
    "    Compute the choice probabilities for each time period and product for a given theta, for each possible state\n",
    "    There are 4 possible states (individuals are never in state 0). For a given theta, compute the choice probabilities for each state\n",
    "    Should a return a (T, J, J+1) array. That is, for each period, for each possible state, the choice probas\n",
    "    \"\"\"\n",
    "    theta_jnp = jnp.array(theta).flatten()\n",
    "    betas = theta_jnp[:-2]\n",
    "    eta = theta_jnp[-2]\n",
    "    gamma = theta_jnp[-1]\n",
    "    \n",
    "    #possible states: 0, 1, 2, 3, 4 \n",
    "    v_1to4_utility_state0 = (betas + eta * prices_50_by_4_jnp).reshape(50, 1, 4)\n",
    "    v_1to4_utility_state1to4 = (betas + eta * prices_50_by_4_jnp).reshape(50, 1, 4) + gamma * jnp.eye(4)\n",
    "    v_utility = jnp.concatenate((v_1to4_utility_state0, v_1to4_utility_state1to4), axis=1)\n",
    "    v_default = jnp.zeros((50, 5, 1))\n",
    "    v_utility_full = jnp.concatenate((v_default, v_utility), axis=2)\n",
    "\n",
    "    # Compute choice probabilities \n",
    "    log_sumexps = logsumexp(v_utility_full, axis=2, keepdims=True)\n",
    "    probas = jnp.exp(v_utility_full - log_sumexps) #get the choice probabilities for each time period and product\n",
    "\n",
    "    return probas\n",
    "ccp_vec = vmap(ccp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood(theta): #(log)-likelihood function\n",
    "    probas_theta = ccp(theta) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "grad_likelihood = jit(grad(likelihood)) ## gradient of the likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_adam(f, grad_f, x0, norm=1e9, tol=0.1, lr=0.05, maxiter=1000, verbose=0, *args): ## generic adam optimizer\n",
    "  \"\"\"\n",
    "  Generic Adam Optimizer. Specify a function f, a starting point x0, possibly a \\n\n",
    "  learning rate in (0, 1). The lower the learning rate, the more stable (and slow) the convergence.\n",
    "  \"\"\"\n",
    "  tic = time.time()\n",
    "  solver = optax.adam(learning_rate=lr)\n",
    "  params = jnp.array(x0, dtype=jnp.float32)\n",
    "  opt_state = solver.init(params)\n",
    "  iternum = 0\n",
    "  while norm > tol and iternum < maxiter :\n",
    "    iternum += 1\n",
    "    grad = grad_f(params, *args)\n",
    "    updates, opt_state = solver.update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    params = jnp.asarray(params, dtype=jnp.float32)\n",
    "    norm = jnp.max(jnp.abs(grad))\n",
    "    if verbose > 0:\n",
    "      if iternum % 100 == 0:\n",
    "        print(f\"Iteration: {iternum}  Norm: {norm}  theta: {jnp.round(params, 2)}\")\n",
    "    if verbose > 1:\n",
    "      print(f\"Iteration: {iternum}  Norm: {norm}  theta: {jnp.round(params, 2)}\")\n",
    "  tac = time.time()\n",
    "  if iternum == maxiter:\n",
    "    print(f\"Convergence not reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "  else:\n",
    "    print(f\"Convergence reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 2517 iterations. \n",
      "Time: 29.813474893569946 seconds. Norm: 0.09705352783203125\n"
     ]
    }
   ],
   "source": [
    "theta_MLE_homo = minimize_adam(likelihood, grad_likelihood, jnp.zeros(6), lr=0.01, verbose=0, maxiter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5353696  -1.2799662  -2.3902292  -1.5117126  -0.35722518  2.5106406 ]\n"
     ]
    }
   ],
   "source": [
    "print(theta_MLE_homo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computation of standard errors\n",
    "\n",
    "@jit\n",
    "def likelihood_it(theta, i, t):\n",
    "    \"\"\"\n",
    "    Computes the likelihood for an individual observation\n",
    "    \"\"\"\n",
    "    probas_theta = ccp(theta)\n",
    "    likelihood_it = jnp.log(probas_theta[t, state_matrix_jnp[t, i], choice_jnp[t, i]])\n",
    "    return likelihood_it\n",
    "\n",
    "grad_likelihood_it = jit(grad(likelihood_it)) ### Takes the gradient of the individual likelihood\n",
    "\n",
    "@jit\n",
    "def outer_grad_likelihood(theta, i, t):\n",
    "    \"\"\"\n",
    "    Takes the outer product (column vector x row vector) of the gradient of the individual likelihood\n",
    "    \"\"\"\n",
    "    grad_it = (grad_likelihood_it(theta, i, t)).reshape(-1, 1) \n",
    "    return grad_it@grad_it.T\n",
    "\n",
    "\n",
    "#computes the outer product above for each individual and time period\n",
    "grad_likelihood_it_vec = vmap(vmap(outer_grad_likelihood, in_axes=(None, 0, None)), in_axes=(None, None, 0)) \n",
    "\n",
    "@jit\n",
    "def compute_standard_errors(theta):\n",
    "    sum_outers = (1/(N*50))*(jnp.sum(grad_likelihood_it_vec(theta, jnp.arange(N), jnp.arange(T)), axis=(0, 1)))\n",
    "    return jnp.diag(jnp.sqrt(jnp.linalg.inv(sum_outers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 5.812934 , 12.313161 ,  5.165976 , 13.472887 ,  5.1963058,\n",
       "        1.7994431], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se = compute_standard_errors(theta_MLE_homo)\n",
    "se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -8.348304  -13.593127   -7.5562053 -14.9846     -5.5535307   0.7111975]\n",
      "[ 3.2775643 11.033195   2.7757468 11.961174   4.839081   4.310084 ]\n"
     ]
    }
   ],
   "source": [
    "###Two classes: instead of estimating theta, we want to estimate the weights phi_1, phi_2 of each class (?)\n",
    "theta_k1 = theta_MLE_homo - se\n",
    "theta_k2 = theta_MLE_homo + se\n",
    "print(theta_k1)\n",
    "print(theta_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def choice_probas_2classes(phi1):\n",
    "    phi2 = 1 - phi1\n",
    "    probas_k1 = ccp(theta_k1)\n",
    "    probas_k2 = ccp(theta_k2)\n",
    "    probas = phi1 * probas_k1 + phi2 * probas_k2\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood_2classes(phi1): #(log)-likelihood function\n",
    "    probas_theta = choice_probas_2classes(phi1) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "\n",
    "grad_likelihood_2classes = jit(grad(likelihood_2classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 204 iterations. \n",
      "Time: 2.9518580436706543 seconds. Norm: 0.08203125\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_1^h$: [ -8.348304  -13.593127   -7.5562053 -14.9846     -5.5535307   0.7111975]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_2^h$: [ 3.2775643 11.033195   2.7757468 11.961174   4.839081   4.310084 ]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (0.6556161046028137, 0.3443838953971863)\n"
     ]
    }
   ],
   "source": [
    "weight_1 = minimize_adam(likelihood_2classes, grad_likelihood_2classes, 0.5, verbose=False)\n",
    "display(Latex(f'$\\Theta_1^h$: {theta_k1}'))\n",
    "display(Latex(f'$\\Theta_2^h$: {theta_k2}'))\n",
    "print(f'Weights: {weight_1.item(), 1-weight_1.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE assuming that $\\Theta^h$ has a normal distribution across the households "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of Monte Carlo draws\n",
    "S = 1000  # Number of simulation draws\n",
    "\n",
    "# Generate random draws for Monte Carlo integration\n",
    "key = jax.random.PRNGKey(123)\n",
    "mc_draws = jax.random.normal(key, (S, 6))  # 5 parameters (4 betas + 1 eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood(theta):\n",
    "    mu = theta[:6]  # Mean of the random parameters\n",
    "    sigma = jnp.diag(jnp.exp(theta[6:]))\n",
    "    betas_eta = mu + jnp.dot(mc_draws, sigma.T)\n",
    "    \n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    \n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood = jit(grad(mixed_logit_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 1204.3084716796875  theta: [-2.95       -1.18       -2.62       -1.74       -0.61        2.9199998\n",
      "  0.39       -0.24        0.39999998 -0.19999999 -0.31        0.06      ]\n",
      "Iteration: 200  Norm: 657.1005859375  theta: [-3.51       -1.13       -3.11       -1.63       -0.64        3.11\n",
      "  0.65999997 -0.38        0.7        -0.31       -0.59        0.69      ]\n",
      "Iteration: 300  Norm: 364.98651123046875  theta: [-3.9299998  -1.06       -3.46       -1.54       -0.66999996  3.1999998\n",
      "  0.82       -0.44        0.84999996 -0.41       -0.77        0.94      ]\n",
      "Iteration: 400  Norm: 232.00759887695312  theta: [-4.19       -1.02       -3.6999998  -1.4599999  -0.68        3.26\n",
      "  0.90999997 -0.47        0.94       -0.48       -0.9         1.04      ]\n",
      "Iteration: 500  Norm: 158.82913208007812  theta: [-4.38       -0.98999995 -3.87       -1.41       -0.68        3.31\n",
      "  0.96999997 -0.45999998  0.98999995 -0.53999996 -1.          1.13      ]\n",
      "Iteration: 600  Norm: 115.94561767578125  theta: [-4.5299997  -0.96999997 -4.         -1.37       -0.69        3.34\n",
      "  1.01       -0.44        1.03       -0.59999996 -1.0799999   1.1899999 ]\n",
      "Iteration: 700  Norm: 88.8949203491211  theta: [-4.65       -0.96       -4.1        -1.3399999  -0.69        3.36\n",
      "  1.04       -0.39999998  1.0699999  -0.65       -1.15        1.24      ]\n",
      "Iteration: 800  Norm: 70.8244857788086  theta: [-4.75      -0.96      -4.18      -1.3199999 -0.69       3.3799999\n",
      "  1.06      -0.37       1.09      -0.69      -1.2099999  1.28     ]\n",
      "Iteration: 900  Norm: 58.287960052490234  theta: [-4.83       -0.96999997 -4.25       -1.3        -0.69        3.3999999\n",
      "  1.0799999  -0.32        1.11       -0.72999996 -1.26        1.31      ]\n",
      "Iteration: 1000  Norm: 49.415130615234375  theta: [-4.9        -0.97999996 -4.31       -1.29       -0.69        3.4199998\n",
      "  1.1        -0.26        1.12       -0.76       -1.3         1.3299999 ]\n",
      "Iteration: 1100  Norm: 43.13328552246094  theta: [-4.94       -1.01       -4.35       -1.29       -0.68        3.4499998\n",
      "  1.11       -0.19        1.13       -0.78999996 -1.3399999   1.3299999 ]\n",
      "Iteration: 1200  Norm: 38.796329498291016  theta: [-4.98       -1.06       -4.39       -1.28       -0.68        3.47\n",
      "  1.12       -0.09999999  1.14       -0.82       -1.38        1.3199999 ]\n",
      "Iteration: 1300  Norm: 35.92893981933594  theta: [-4.99 -1.13 -4.41 -1.29 -0.68  3.51  1.12  0.    1.15 -0.84 -1.42  1.3 ]\n",
      "Iteration: 1400  Norm: 33.93706130981445  theta: [-4.99       -1.22       -4.42       -1.29       -0.66999996  3.55\n",
      "  1.12        0.12        1.15       -0.84999996 -1.4599999   1.25      ]\n",
      "Iteration: 1500  Norm: 31.992748260498047  theta: [-4.97       -1.3199999  -4.42       -1.31       -0.66999996  3.61\n",
      "  1.11        0.22        1.14       -0.85999995 -1.5         1.18      ]\n",
      "Iteration: 1600  Norm: 29.708160400390625  theta: [-4.94       -1.4        -4.41       -1.3299999  -0.66999996  3.6499999\n",
      "  1.11        0.29        1.14       -0.84999996 -1.54        1.12      ]\n",
      "Iteration: 1700  Norm: 27.509410858154297  theta: [-4.91       -1.4599999  -4.4        -1.35       -0.65999997  3.6699998\n",
      "  1.1         0.32999998  1.13       -0.83       -1.5799999   1.0699999 ]\n",
      "Iteration: 1800  Norm: 25.715269088745117  theta: [-4.89       -1.5        -4.39       -1.38       -0.65        3.6699998\n",
      "  1.09        0.34        1.12       -0.79999995 -1.62        1.04      ]\n",
      "Iteration: 1900  Norm: 24.299009323120117  theta: [-4.8599997 -1.53      -4.37      -1.4       -0.64       3.6699998\n",
      "  1.0799999  0.35       1.12      -0.76      -1.65       1.01     ]\n",
      "Iteration: 2000  Norm: 23.179655075073242  theta: [-4.83       -1.55       -4.35       -1.43       -0.63        3.6699998\n",
      "  1.0699999   0.35        1.11       -0.71999997 -1.6899999   0.96999997]\n",
      "Iteration: 2100  Norm: 22.30971908569336  theta: [-4.7999997  -1.5799999  -4.33       -1.4599999  -0.62        3.6599998\n",
      "  1.06        0.35        1.1        -0.66999996 -1.73        0.93      ]\n",
      "Iteration: 2200  Norm: 21.67078399658203  theta: [-4.75       -1.61       -4.2999997  -1.49       -0.61        3.6599998\n",
      "  1.05        0.35999998  1.09       -0.61       -1.77        0.88      ]\n",
      "Iteration: 2300  Norm: 21.27138328552246  theta: [-4.71       -1.63       -4.27       -1.53       -0.59999996  3.6599998\n",
      "  1.04        0.35999998  1.0799999  -0.53999996 -1.81        0.82      ]\n",
      "Iteration: 2400  Norm: 21.164260864257812  theta: [-4.65       -1.67       -4.24       -1.5699999  -0.59        3.6599998\n",
      "  1.02        0.37        1.06       -0.45       -1.86        0.72999996]\n",
      "Iteration: 2500  Norm: 21.51043128967285  theta: [-4.6       -1.7099999 -4.2       -1.62      -0.59       3.6699998\n",
      "  1.01       0.39       1.05      -0.35      -1.9        0.61     ]\n",
      "Iteration: 2600  Norm: 22.81363296508789  theta: [-4.5299997  -1.77       -4.17       -1.6899999  -0.58        3.6899998\n",
      "  0.98999995  0.41        1.03       -0.22999999 -1.9499999   0.39999998]\n",
      "Iteration: 2700  Norm: 25.596607208251953  theta: [-4.47       -1.87       -4.14       -1.79       -0.56        3.74\n",
      "  0.96999997  0.45        1.         -0.08       -2.01       -0.13      ]\n",
      "Iteration: 2800  Norm: 24.19008445739746  theta: [-4.5299997  -1.99       -4.19       -1.92       -0.55        3.82\n",
      "  0.98999995  0.48999998  1.          0.05       -2.07       -0.89      ]\n",
      "Iteration: 2900  Norm: 20.43492317199707  theta: [-4.66       -2.09       -4.2999997  -2.02       -0.53999996  3.9199998\n",
      "  1.03        0.53        1.03        0.14999999 -2.1299999  -1.36      ]\n",
      "Iteration: 3000  Norm: 17.071420669555664  theta: [-4.81      -2.18      -4.43      -2.11      -0.55       4.0299997\n",
      "  1.06       0.58       1.06       0.24      -2.18      -1.6899999]\n",
      "Iteration: 3100  Norm: 14.317081451416016  theta: [-4.96       -2.26       -4.5699997  -2.2        -0.56        4.15\n",
      "  1.1         0.63        1.1         0.32999998 -2.23       -1.9399999 ]\n",
      "Iteration: 3200  Norm: 12.102989196777344  theta: [-5.12       -2.34       -4.71       -2.28       -0.57        4.2799997\n",
      "  1.14        0.68        1.13        0.39999998 -2.27       -2.1499999 ]\n",
      "Iteration: 3300  Norm: 11.323457717895508  theta: [-5.2799997  -2.4199998  -4.85       -2.36       -0.58        4.4\n",
      "  1.18        0.71999997  1.17        0.47       -2.31       -2.33      ]\n",
      "Iteration: 3400  Norm: 10.840629577636719  theta: [-5.44       -2.5        -5.         -2.44       -0.59999996  4.5299997\n",
      "  1.2099999   0.77        1.1999999   0.53       -2.34       -2.49      ]\n",
      "Iteration: 3500  Norm: 10.320527076721191  theta: [-5.6       -2.58      -5.14      -2.52      -0.61       4.66\n",
      "  1.25       0.81       1.23       0.58      -2.37      -2.6399999]\n",
      "Iteration: 3600  Norm: 9.78515625  theta: [-5.7599998  -2.6599998  -5.29       -2.59       -0.62        4.7799997\n",
      "  1.28        0.84999996  1.26        0.63       -2.3999999  -2.77      ]\n",
      "Iteration: 3700  Norm: 9.236550331115723  theta: [-5.91      -2.74      -5.43      -2.6699998 -0.64       4.91\n",
      "  1.31       0.88       1.3        0.68      -2.4199998 -2.8899999]\n",
      "Iteration: 3800  Norm: 8.734932899475098  theta: [-6.0699997  -2.82       -5.58       -2.75       -0.65        5.0299997\n",
      "  1.3399999   0.91999996  1.3199999   0.71999997 -2.45       -3.01      ]\n",
      "Iteration: 3900  Norm: 9.641079902648926  theta: [-6.23       -2.8999999  -5.72       -2.83       -0.65999997  5.16\n",
      "  1.37        0.95        1.35        0.76       -2.47       -3.12      ]\n",
      "Iteration: 4000  Norm: 7.758230686187744  theta: [-6.3799996  -2.98       -5.8599997  -2.8999999  -0.68        5.2799997\n",
      "  1.4         0.97999996  1.38        0.79999995 -2.49       -3.22      ]\n",
      "Iteration: 4100  Norm: 7.326177597045898  theta: [-6.5299997 -3.05      -6.        -2.98      -0.69       5.4\n",
      "  1.43       1.01       1.4        0.83      -2.51      -3.31     ]\n",
      "Iteration: 4200  Norm: 6.903400421142578  theta: [-6.68      -3.1299999 -6.14      -3.05      -0.7        5.52\n",
      "  1.4499999  1.04       1.43       0.87      -2.53      -3.3999999]\n",
      "Iteration: 4300  Norm: 6.490131378173828  theta: [-6.83       -3.1999998  -6.27       -3.1299999  -0.71999997  5.64\n",
      "  1.48        1.0699999   1.4499999   0.9        -2.55       -3.49      ]\n",
      "Iteration: 4400  Norm: 6.102447509765625  theta: [-6.98       -3.28       -6.41       -3.1999998  -0.72999996  5.75\n",
      "  1.5         1.09        1.48        0.93       -2.57       -3.58      ]\n",
      "Iteration: 4500  Norm: 5.772745132446289  theta: [-7.12      -3.35      -6.54      -3.27      -0.74       5.87\n",
      "  1.53       1.12       1.5        0.95      -2.58      -3.6599998]\n",
      "Iteration: 4600  Norm: 5.456325531005859  theta: [-7.27       -3.4199998  -6.67       -3.34       -0.75        5.98\n",
      "  1.55        1.14        1.52        0.97999996 -2.6        -3.73      ]\n",
      "Iteration: 4700  Norm: 5.185727119445801  theta: [-7.41      -3.49      -6.7999997 -3.4099998 -0.77       6.1\n",
      "  1.5699999  1.17       1.54       1.01      -2.62      -3.81     ]\n",
      "Iteration: 4800  Norm: 4.868078231811523  theta: [-7.5499997 -3.57      -6.93      -3.48      -0.78       6.21\n",
      "  1.5899999  1.1899999  1.56       1.03      -2.6299999 -3.8799999]\n",
      "Iteration: 4900  Norm: 4.597663402557373  theta: [-7.69       -3.6399999  -7.06       -3.55       -0.78999996  6.3199997\n",
      "  1.61        1.2099999   1.5799999   1.06       -2.6499999  -3.9499998 ]\n",
      "Iteration: 5000  Norm: 4.355354309082031  theta: [-7.83       -3.7099998  -7.19       -3.62       -0.79999995  6.43\n",
      "  1.63        1.23        1.5999999   1.0799999  -2.6599998  -4.02      ]\n",
      "Convergence not reached after 5000 iterations. \n",
      "Time: 126.68640804290771 seconds. Norm: 4.355354309082031\n"
     ]
    }
   ],
   "source": [
    "### This does not converge properly and as the norm reduces, the bias is still very high\n",
    "x_start = jnp.concatenate((theta_MLE_homo, 0.1*jnp.ones(6)))\n",
    "theta_mixed_logit = minimize_adam(mixed_logit_likelihood, grad_mixed_logit_likelihood, x_start, lr=0.005, maxiter=5000, verbose=1, tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-6.116932  , -4.1460795 , -5.8156753 , -2.709583  ,  0.42825902,\n",
       "        5.4318976 ], dtype=float32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_mixed_logit[:6] - mu.flatten() #quite strong bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_matrix_np2 = np.array(jnp.vstack((jnp.zeros((1, 1000)), state_matrix_jnp[1:, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "###rerun the state based on choices. Takes 9 secs.\n",
    "for i in range(N):\n",
    "    for t in range(49):\n",
    "        choice_it = choice_jnp[t, i]\n",
    "        if choice_it != 0:\n",
    "            state_matrix_np2[t+1, i] = choice_it\n",
    "        else:\n",
    "            state_matrix_np2[t+1, i] = state_matrix_np2[t, i]\n",
    "state_matrix_jnp2 = jnp.array(state_matrix_np2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood2(theta):\n",
    "    mu = theta[:6]  # Mean of the random parameters\n",
    "    sigma = jnp.diag(jnp.exp(theta[6:]))\n",
    "    betas_eta = mu + jnp.dot(mc_draws, sigma.T)\n",
    "    \n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    \n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp2, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood2 = jit(grad(mixed_logit_likelihood2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 87.0124282836914  theta: [-3.8899999 -0.53      -3.54      -0.9       -0.7        3.1799998\n",
      "  0.87      -2.32       0.93      -2.3999999 -2.75       1.05     ]\n",
      "Iteration: 200  Norm: 0.744306206703186  theta: [-4.1        -0.48       -3.6799998  -0.87       -0.71999997  3.24\n",
      "  0.93       -2.8899999   0.97999996 -2.51       -2.8899999   1.18      ]\n",
      "Iteration: 300  Norm: 0.4810718894004822  theta: [-4.1        -0.48       -3.6799998  -0.87       -0.71999997  3.24\n",
      "  0.93       -3.4199998   0.97999996 -2.56       -2.96        1.18      ]\n",
      "Iteration: 400  Norm: 0.32755860686302185  theta: [-4.0899997  -0.48       -3.6699998  -0.87       -0.71999997  3.23\n",
      "  0.93       -3.85        0.97999996 -2.61       -3.01        1.17      ]\n",
      "Iteration: 500  Norm: 0.23669716715812683  theta: [-4.0899997  -0.48       -3.6699998  -0.87       -0.71999997  3.23\n",
      "  0.93       -4.2         0.97999996 -2.6399999  -3.04        1.17      ]\n",
      "Iteration: 600  Norm: 57.805503845214844  theta: [-4.13       -0.48       -3.6999998  -0.87       -0.71999997  3.24\n",
      "  0.94       -4.49        0.98999995 -2.6599998  -3.03        1.1899999 ]\n",
      "Iteration: 700  Norm: 1.4523234367370605  theta: [-4.0899997  -0.48       -3.6699998  -0.87       -0.71999997  3.23\n",
      "  0.93       -4.74        0.97999996 -2.6799998  -3.05        1.17      ]\n",
      "Convergence reached after 703 iterations. \n",
      "Time: 18.000570058822632 seconds. Norm: 0.1539444923400879\n"
     ]
    }
   ],
   "source": [
    "### This does not converge properly and as the norm reduces, the bias is still very high\n",
    "x_start = jnp.concatenate((theta_MLE_homo, 0.1*jnp.ones(6)))\n",
    "theta_mixed_logit2 = minimize_adam(mixed_logit_likelihood2, grad_mixed_logit_likelihood2, x_start, lr=0.5, maxiter=10_000, verbose=1, tol=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-6.116932  , -4.1460795 , -5.8156753 , -2.709583  ,  0.42825902,\n",
       "        5.4318976 ], dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_mixed_logit[:6] - mu.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-2.3790617 , -0.92039466, -2.3002887 ,  0.04121357,  0.5106937 ,\n",
       "        2.2324889 ], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_mixed_logit2[:6] - mu.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.71,  0.44, -1.37, -0.91, -1.23,  1.  ])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EIO_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "from IPython.display import display, Latex\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.71]\n",
      " [ 0.44]\n",
      " [-1.37]\n",
      " [-0.91]\n",
      " [-1.23]\n",
      " [ 1.  ]] \n",
      "\n",
      "[[3.22 0.   0.   0.   0.   0.  ]\n",
      " [0.   3.24 0.   0.   0.   0.  ]\n",
      " [0.   0.   2.87 0.   0.   0.  ]\n",
      " [0.   0.   0.   4.15 0.   0.  ]\n",
      " [0.   0.   0.   0.   1.38 0.  ]\n",
      " [0.   0.   0.   0.   0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "N = 1000\n",
    "J = 4\n",
    "T = 150\n",
    "\n",
    "# Generate the data\n",
    "np.random.seed(123)\n",
    "mu = np.array([-1.71, 0.44, -1.37, -0.91, -1.23, 1]).reshape(-1, 1)\n",
    "sigma = np.diag(np.array([3.22, 3.24, 2.87, 4.15, 1.38, 1])).reshape(6, 6)\n",
    "\n",
    "#sigma = np.diag(np.zeros(6)).reshape(6, 6) #This was run as a test for theta MLE homogeneous\n",
    "# Under the \"correct\" distributional assumption, that is if we generate homogeneous consumers,\n",
    "# our MLE hmg estimate for Theta, including gamma, is very well identified. So any problem in \n",
    "# identification encountered later is driven by consumer heterogeneity as a confounder.\n",
    "\n",
    "print(mu, '\\n')\n",
    "print(sigma)\n",
    "\n",
    "\n",
    "# generate the random parameters\n",
    "betas = np.random.multivariate_normal(mu.flatten(), sigma, N)\n",
    "betas_np = betas[:, :-2]\n",
    "etas_np = betas[:, -2]\n",
    "gammas_np = betas[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_transition_states = pd.read_csv(r'price_transition_states.csv')\n",
    "price_transition_matrix = pd.read_csv(r'transition_prob_matrix.csv')\n",
    "price_transition_matrix_np = price_transition_matrix.to_numpy()\n",
    "price_transition_states_np = price_transition_states.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_prices(states, transition, T):\n",
    "    state_indices = np.arange(states.shape[0])\n",
    "\n",
    "    price_simu = np.zeros((T, 6)) #create a matrix to store the simulated prices\n",
    "    price_simu[0] = states[0] #fix the initial vector of prices\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        preceding_state = price_simu[t-1, :] #take the preceding state\n",
    "        index_preceding_state = int(preceding_state[-1] - 1) #take the index of the preceding state (-1 for 0-indexing in Python)\n",
    "        index_next_state = np.random.choice(state_indices, p=(transition[index_preceding_state, :].flatten())) #draw the next state\n",
    "        price_simu[t, :] = states[index_next_state] #update the price vector and store it\n",
    "    return price_simu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_150_by_6 = simulate_prices(price_transition_states_np, price_transition_matrix_np, T)\n",
    "prices_150_by_4 = price_150_by_6[:, :-2] #remove the indices column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate baseline utility data (no loyalty)\n",
    "utility_np = np.zeros((T, 1+J, N)) # 1 for the outside option, J for the number of products\n",
    "for t in range(1, T):\n",
    "    for i in range(N):\n",
    "        utility_np[t, 0, i] = np.random.gumbel() #outside option, just a random noise\n",
    "        utility_np[t, 1:, i] = betas_np[i, :] + etas_np[i]*prices_150_by_4[t, :] + np.random.gumbel(size=J) #utility for the J products\n",
    "\n",
    "#utility_np_orig = utility_np.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add loyalty\n",
    "state_matrix = np.zeros((T, N), dtype=int) #the state at time 0 is 0\n",
    "state_matrix[1, :] = np.argmax(utility_np[0, :, :], axis=0) #initialize the state simulation\n",
    "\n",
    "for t in range(1, T-1):\n",
    "    for i in range(N):\n",
    "        state_it = state_matrix[t, i]\n",
    "        for j in range(1, J+1): #exclude the outside option\n",
    "            utility_np[t, j, i] += gammas_np[i] * (j == state_it)\n",
    "        choice = np.argmax(utility_np[t, :, i])\n",
    "        if choice==0:\n",
    "            state_matrix[t+1, i] = state_it ### if the outside option is chosen, the state remains the same\n",
    "        else:\n",
    "            state_matrix[t+1, i] = choice ### if a product is chosen, the state is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility_orig_jnp = jnp.array(utility_np_orig[100:, :, :])  #50 x 5 x 1000\n",
    "utility_jnp = jnp.array(utility_np[100:, :, :])            #50 x 5 x 1000\n",
    "choice_jnp = jnp.argmax(utility_np, axis=1)[100:, :]       #50 x 1000\n",
    "prices_50_by_4_jnp = jnp.array(prices_150_by_4[100:, :])   #50 x 4\n",
    "state_matrix_jnp = jnp.array(state_matrix[100:, :])        #50 x 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i: Homogeneous, Two classes, and Mixed Logit\n",
    "* Homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def ccp(theta):\n",
    "    \"\"\"\n",
    "    Compute the choice probabilities for each time period and product for a given theta, for each possible state\n",
    "    There are 4 possible states (individuals are never in state 0). For a given theta, compute the choice probabilities for each state\n",
    "    Should a return a (T, J, J+1) array. That is, for each period, for each possible state, the choice probas\n",
    "    \"\"\"\n",
    "    theta_jnp = jnp.array(theta).flatten()\n",
    "    betas = theta_jnp[:-2]\n",
    "    eta = theta_jnp[-2]\n",
    "    gamma = theta_jnp[-1]\n",
    "    \n",
    "    #possible states: 0, 1, 2, 3, 4 \n",
    "    v_1to4_utility_state0 = (betas + eta * prices_50_by_4_jnp).reshape(50, 1, 4)\n",
    "    v_1to4_utility_state1to4 = (betas + eta * prices_50_by_4_jnp).reshape(50, 1, 4) + gamma * jnp.eye(4)\n",
    "    v_utility = jnp.concatenate((v_1to4_utility_state0, v_1to4_utility_state1to4), axis=1)\n",
    "    v_default = jnp.zeros((50, 5, 1))\n",
    "    v_utility_full = jnp.concatenate((v_default, v_utility), axis=2)\n",
    "\n",
    "    # Compute choice probabilities \n",
    "    log_sumexps = logsumexp(v_utility_full, axis=2, keepdims=True)\n",
    "    probas = jnp.exp(v_utility_full - log_sumexps) #get the choice probabilities for each time period and product\n",
    "\n",
    "    return probas\n",
    "ccp_vec = vmap(ccp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood(theta): #(log)-likelihood function\n",
    "    probas_theta = ccp(theta) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "grad_likelihood = jit(grad(likelihood)) ## gradient of the likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_adam(f, grad_f, x0, norm=1e9, tol=0.1, lr=0.05, maxiter=1000, verbose=0, *args): ## generic adam optimizer\n",
    "  \"\"\"\n",
    "  Generic Adam Optimizer. Specify a function f, a starting point x0, possibly a \\n\n",
    "  learning rate in (0, 1). The lower the learning rate, the more stable (and slow) the convergence.\n",
    "  \"\"\"\n",
    "  tic = time.time()\n",
    "  solver = optax.adam(learning_rate=lr)\n",
    "  params = jnp.array(x0, dtype=jnp.float32)\n",
    "  opt_state = solver.init(params)\n",
    "  iternum = 0\n",
    "  while norm > tol and iternum < maxiter :\n",
    "    iternum += 1\n",
    "    grad = grad_f(params, *args)\n",
    "    updates, opt_state = solver.update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    params = jnp.asarray(params, dtype=jnp.float32)\n",
    "    norm = jnp.max(jnp.abs(grad))\n",
    "    if verbose > 0:\n",
    "      if iternum % 100 == 0:\n",
    "        print(f\"Iteration: {iternum}  Norm: {norm}  theta: {jnp.round(params, 2)}\")\n",
    "    if verbose > 1:\n",
    "      print(f\"Iteration: {iternum}  Norm: {norm}  theta: {jnp.round(params, 2)}\")\n",
    "  tac = time.time()\n",
    "  if iternum == maxiter:\n",
    "    print(f\"Convergence not reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "  else:\n",
    "    print(f\"Convergence reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 2517 iterations. \n",
      "Time: 30.358713388442993 seconds. Norm: 0.09705352783203125\n"
     ]
    }
   ],
   "source": [
    "theta_MLE_homo = minimize_adam(likelihood, grad_likelihood, jnp.zeros(6), lr=0.01, verbose=0, maxiter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5353696  -1.2799662  -2.3902292  -1.5117126  -0.35722518  2.5106406 ]\n"
     ]
    }
   ],
   "source": [
    "print(theta_MLE_homo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computation of standard errors\n",
    "\n",
    "@jit\n",
    "def likelihood_it(theta, i, t):\n",
    "    \"\"\"\n",
    "    Computes the likelihood for an individual observation\n",
    "    \"\"\"\n",
    "    probas_theta = ccp(theta)\n",
    "    likelihood_it = jnp.log(probas_theta[t, state_matrix_jnp[t, i], choice_jnp[t, i]])\n",
    "    return likelihood_it\n",
    "\n",
    "grad_likelihood_it = jit(grad(likelihood_it)) ### Takes the gradient of the individual likelihood\n",
    "\n",
    "@jit\n",
    "def outer_grad_likelihood(theta, i, t):\n",
    "    \"\"\"\n",
    "    Takes the outer product (column vector x row vector) of the gradient of the individual likelihood\n",
    "    \"\"\"\n",
    "    grad_it = (grad_likelihood_it(theta, i, t)).reshape(-1, 1) \n",
    "    return grad_it@grad_it.T\n",
    "\n",
    "\n",
    "#computes the outer product above for each individual and time period\n",
    "grad_likelihood_it_vec = vmap(vmap(outer_grad_likelihood, in_axes=(None, 0, None)), in_axes=(None, None, 0)) \n",
    "\n",
    "@jit\n",
    "def compute_standard_errors(theta):\n",
    "    sum_outers = (1/(N*50))*(jnp.sum(grad_likelihood_it_vec(theta, jnp.arange(N), jnp.arange(T)), axis=(0, 1)))\n",
    "    return jnp.diag(jnp.sqrt(jnp.linalg.inv(sum_outers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 5.812934 , 12.313161 ,  5.165976 , 13.472887 ,  5.1963058,\n",
       "        1.7994431], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se = compute_standard_errors(theta_MLE_homo)\n",
    "se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLE 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -8.348304  -13.593127   -7.5562053 -14.9846     -5.5535307   0.7111975]\n",
      "[ 3.2775643 11.033195   2.7757468 11.961174   4.839081   4.310084 ]\n"
     ]
    }
   ],
   "source": [
    "###Two classes: instead of estimating theta, we want to estimate the weights phi_1, phi_2 of each class (?)\n",
    "theta_k1 = theta_MLE_homo - se\n",
    "theta_k2 = theta_MLE_homo + se\n",
    "print(theta_k1)\n",
    "print(theta_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def choice_probas_2classes(phi1):\n",
    "    phi2 = 1 - phi1\n",
    "    probas_k1 = ccp(theta_k1)\n",
    "    probas_k2 = ccp(theta_k2)\n",
    "    probas = phi1 * probas_k1 + phi2 * probas_k2\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood_2classes(phi1): #(log)-likelihood function\n",
    "    probas_theta = choice_probas_2classes(phi1) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "\n",
    "grad_likelihood_2classes = jit(grad(likelihood_2classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 204 iterations. \n",
      "Time: 2.940648317337036 seconds. Norm: 0.08203125\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_1^h$: [ -8.348304  -13.593127   -7.5562053 -14.9846     -5.5535307   0.7111975]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_2^h$: [ 3.2775643 11.033195   2.7757468 11.961174   4.839081   4.310084 ]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (0.6556161046028137, 0.3443838953971863)\n"
     ]
    }
   ],
   "source": [
    "weight_1 = minimize_adam(likelihood_2classes, grad_likelihood_2classes, 0.5, verbose=False)\n",
    "display(Latex(f'$\\Theta_1^h$: {theta_k1}'))\n",
    "display(Latex(f'$\\Theta_2^h$: {theta_k2}'))\n",
    "print(f'Weights: {weight_1.item(), 1-weight_1.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLE assuming that $\\Theta^h$ has a normal distribution across the households "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: sanity check. Let's assume we know $\\sigma$: do we recover the proper mu under the correct distributional assumption ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of Monte Carlo draws\n",
    "S = 1000  # Number of simulation draws\n",
    "\n",
    "# Generate random draws for Monte Carlo integration\n",
    "key = jax.random.PRNGKey(123)\n",
    "mc_draws = jax.random.normal(key, (S, 6))  # 5 parameters (4 betas + 1 eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood_correct(theta):\n",
    "    mu = theta.flatten()  # Mean of the random parameters\n",
    "    betas_eta = mu + mc_draws * jnp.sqrt(jnp.diag(sigma))\n",
    "    \n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood_correct = jit(grad(mixed_logit_likelihood_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 587 iterations. \n",
      "Time: 15.29751205444336 seconds. Norm: 0.0977182388305664\n",
      "[-3.1571164 -0.8895682 -2.804269  -1.686722  -1.1715082  3.8051717]\n"
     ]
    }
   ],
   "source": [
    "### This does not converge properly and as the norm reduces, the bias is still very high\n",
    "theta_mixed_logit_correct = minimize_adam(mixed_logit_likelihood_correct, \n",
    "                                          grad_mixed_logit_likelihood_correct, \n",
    "                                          theta_MLE_homo, \n",
    "                                          lr=0.05, maxiter=5000, verbose=0, tol=0.1)\n",
    "print(theta_mixed_logit_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is bias. Maybe this is to be expected given the instructions of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood(theta):\n",
    "    mu = theta[:6]  # Mean of the random parameters\n",
    "    sigma = jnp.exp(theta[6:])\n",
    "    betas_eta = mu + mc_draws * sigma\n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood = jit(grad(mixed_logit_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 21.240543365478516  theta: [-4.47       -0.98999995 -3.98       -1.36       -0.55        3.1\n",
      "  0.96       -2.1699998   1.         -2.22       -2.8799999   1.1999999 ]\n",
      "Iteration: 200  Norm: 0.8320649266242981  theta: [-4.69       -0.96999997 -4.14       -1.36       -0.55        3.1399999\n",
      "  1.02       -2.6499999   1.05       -2.45       -2.98        1.3       ]\n",
      "Iteration: 300  Norm: 0.579755961894989  theta: [-4.77       -0.96999997 -4.2        -1.37       -0.55        3.1599998\n",
      "  1.04       -3.1399999   1.06       -2.6        -3.04        1.3399999 ]\n",
      "Iteration: 400  Norm: 0.4046667218208313  theta: [-4.79       -0.96999997 -4.21       -1.37       -0.55        3.1599998\n",
      "  1.05       -3.55        1.0699999  -2.72       -3.08        1.35      ]\n",
      "Iteration: 500  Norm: 0.295127809047699  theta: [-4.79       -0.96999997 -4.21       -1.37       -0.55        3.1599998\n",
      "  1.05       -3.8999999   1.0699999  -2.82       -3.12        1.3399999 ]\n",
      "Iteration: 600  Norm: 0.22442765533924103  theta: [-4.7799997  -0.96999997 -4.21       -1.37       -0.55        3.1599998\n",
      "  1.04       -4.19        1.0699999  -2.8999999  -3.1499999   1.3399999 ]\n",
      "Convergence reached after 647 iterations. \n",
      "Time: 16.959004878997803 seconds. Norm: 0.19973911345005035\n"
     ]
    }
   ],
   "source": [
    "### This does not seem to converge well and the bias is high.\n",
    "x_start = jnp.concatenate((theta_MLE_homo, jnp.zeros(6)))\n",
    "theta_mixed_logit = minimize_adam(mixed_logit_likelihood, grad_mixed_logit_likelihood, x_start, lr=0.3, maxiter=1000, verbose=1, tol=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-3.0696974 , -1.4099526 , -2.8347201 , -0.45604497,  0.6823924 ,\n",
       "        2.1601624 ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_mixed_logit[:6] - mu.flatten() #quite strong bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii: Re-initialize the initial state and re-run the state computation given choice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_matrix_np2 = np.array(jnp.vstack((jnp.zeros((1, 1000)), state_matrix_jnp[1:, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "###rerun the state based on choices. Takes 9 secs.\n",
    "for i in range(N):\n",
    "    for t in range(49):\n",
    "        choice_it = choice_jnp[t, i]\n",
    "        if choice_it != 0:\n",
    "            state_matrix_np2[t+1, i] = choice_it\n",
    "        else:\n",
    "            state_matrix_np2[t+1, i] = state_matrix_np2[t, i]\n",
    "state_matrix_jnp2 = jnp.array(state_matrix_np2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, sanity check. Let's assume sigma is known, and see if we can recover the true mu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood_correct2(theta):\n",
    "    mu = theta.flatten()  # Mean of the random parameters\n",
    "    betas_eta = mu + mc_draws * jnp.sqrt(jnp.diag(sigma))\n",
    "    \n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp2, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood_correct2 = jit(grad(mixed_logit_likelihood_correct2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 620 iterations. \n",
      "Time: 15.824934005737305 seconds. Norm: 0.09795951843261719\n",
      "[-2.8360603  -0.38305134 -2.5122223  -1.1973017  -1.3416026   3.8803911 ]\n"
     ]
    }
   ],
   "source": [
    "### Much closer, but still not there, and one sign is off.\n",
    "theta_mixed_logit_correct2 = minimize_adam(mixed_logit_likelihood_correct2, \n",
    "                                          grad_mixed_logit_likelihood_correct2, \n",
    "                                          theta_MLE_homo, \n",
    "                                          lr=0.05, maxiter=5000, verbose=0, tol=0.1)\n",
    "print(theta_mixed_logit_correct2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Mixed Logit estimtion of $\\Theta$ and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood2(theta):\n",
    "    mu = theta[:6]  # Mean of the random parameters\n",
    "    sigma = jnp.exp(theta[6:]) \n",
    "    betas_eta = mu + mc_draws * sigma\n",
    "    \n",
    "    probas_theta = ccp_vec(betas_eta)  # Shape: (S, T, 6)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 6)\n",
    "    \n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(50)[:, None], state_matrix_jnp2, choice_jnp])) \n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood2 = jit(grad(mixed_logit_likelihood2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 17.723278045654297  theta: [-4.17       -0.47       -3.75       -0.85999995 -0.72999996  3.27\n",
      "  0.95       -2.1399999   1.         -1.9        -2.6299999   1.22      ]\n",
      "Iteration: 200  Norm: 0.8716120719909668  theta: [-4.14       -0.47       -3.7099998  -0.87       -0.72999996  3.25\n",
      "  0.95       -2.72        0.98999995 -2.09       -2.75        1.1999999 ]\n",
      "Iteration: 300  Norm: 0.5742689967155457  theta: [-4.12       -0.47       -3.6999998  -0.87       -0.71999997  3.25\n",
      "  0.94       -3.23        0.98999995 -2.25       -2.85        1.1899999 ]\n",
      "Iteration: 400  Norm: 0.39445576071739197  theta: [-4.11       -0.48       -3.6899998  -0.87       -0.71999997  3.24\n",
      "  0.94       -3.6599998   0.97999996 -2.36       -2.9199998   1.18      ]\n",
      "Iteration: 500  Norm: 0.2860749661922455  theta: [-4.1        -0.48       -3.6799998  -0.87       -0.71999997  3.24\n",
      "  0.93       -4.          0.97999996 -2.46       -2.98        1.18      ]\n",
      "Iteration: 600  Norm: 0.21706745028495789  theta: [-4.0899997  -0.48       -3.6799998  -0.87       -0.71999997  3.23\n",
      "  0.93       -4.29        0.97999996 -2.53       -3.02        1.17      ]\n",
      "Convergence reached after 633 iterations. \n",
      "Time: 16.423831939697266 seconds. Norm: 0.19978854060173035\n"
     ]
    }
   ],
   "source": [
    "### This does not converge properly and as the norm reduces, the bias is still very high\n",
    "x_start = jnp.concatenate((theta_MLE_homo, jnp.zeros(6)))\n",
    "theta_mixed_logit2 = minimize_adam(mixed_logit_likelihood2, grad_mixed_logit_likelihood2, x_start, lr=0.3, maxiter=1000, verbose=1, tol=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-3.0696974 , -1.4099526 , -2.8347201 , -0.45604497,  0.6823924 ,\n",
       "        2.1601624 ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_mixed_logit[:6] - mu.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-2.383689  , -0.9195424 , -2.3040013 ,  0.04099923,  0.51023096,\n",
       "        2.2343519 ], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_mixed_logit2[:6] - mu.flatten() #bias is slightly smaller"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EIO_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

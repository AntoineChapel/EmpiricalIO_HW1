{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "from IPython.display import display, Latex\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.71]\n",
      " [ 0.44]\n",
      " [-1.37]\n",
      " [-0.91]\n",
      " [-1.23]] \n",
      "\n",
      "[[3.22 0.   0.   0.   0.  ]\n",
      " [0.   3.24 0.   0.   0.  ]\n",
      " [0.   0.   2.87 0.   0.  ]\n",
      " [0.   0.   0.   4.15 0.  ]\n",
      " [0.   0.   0.   0.   1.38]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "N = 1000\n",
    "J = 4\n",
    "T = 50\n",
    "\n",
    "# Generate the data\n",
    "np.random.seed(123)\n",
    "mu = np.array([-1.71, 0.44, -1.37, -0.91, -1.23]).reshape(-1, 1)\n",
    "sigma = np.diag(np.array([3.22, 3.24, 2.87, 4.15, 1.38])).reshape(5, 5)\n",
    "\n",
    "print(mu, '\\n')\n",
    "print(sigma)\n",
    "\n",
    "\n",
    "# generate the random parameters\n",
    "betas = np.random.multivariate_normal(mu.flatten(), sigma, N)\n",
    "betas_np = betas[:, :-1]\n",
    "etas_np = betas[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X1        X2        X3        X4  X5  id\n",
      "0  0.809830  2.730574  0.770491  2.966650   0   1\n",
      "1  0.785441  2.453663  0.771422  2.964841   0   2\n",
      "2  0.823926  2.238301  0.790901  2.914317   0   3\n",
      "3  1.005571  1.581941  0.918171  2.084867   0   4\n",
      "4  0.798525  2.693030  0.762935  2.690978   0   5\n",
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0  0.050283  0.020251  0.017505  0.003655  0.026568  0.006548  0.006352   \n",
      "1  0.040564  0.023169  0.016224  0.003816  0.024936  0.007435  0.007776   \n",
      "2  0.034444  0.016302  0.015208  0.004359  0.022437  0.006756  0.007631   \n",
      "3  0.017591  0.008223  0.009868  0.007142  0.013897  0.006043  0.011003   \n",
      "4  0.042309  0.016379  0.015619  0.003890  0.027125  0.006523  0.006811   \n",
      "\n",
      "         V8        V9       V10  ...       V91       V92       V93       V94  \\\n",
      "0  0.008375  0.016022  0.013590  ...  0.017482  0.003807  0.008912  0.005710   \n",
      "1  0.006751  0.014843  0.012902  ...  0.020574  0.003186  0.010063  0.005734   \n",
      "2  0.008431  0.019615  0.015308  ...  0.016124  0.004657  0.010655  0.004589   \n",
      "3  0.009424  0.022074  0.012562  ...  0.010919  0.007949  0.013787  0.002806   \n",
      "4  0.008304  0.017501  0.014749  ...  0.016193  0.004160  0.009280  0.004994   \n",
      "\n",
      "        V95       V96       V97       V98       V99      V100  \n",
      "0  0.016266  0.008533  0.006934  0.002275  0.001474  0.008172  \n",
      "1  0.013634  0.008189  0.006430  0.001822  0.001520  0.009950  \n",
      "2  0.014261  0.009842  0.009434  0.002486  0.001853  0.009703  \n",
      "3  0.011120  0.011316  0.016923  0.003837  0.003444  0.015517  \n",
      "4  0.017674  0.008824  0.008212  0.002280  0.001601  0.008316  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "###Extract data\n",
    "\n",
    "price_transition_states = pd.read_csv(r'price_transition_states.csv')\n",
    "price_transition_matrix = pd.read_csv(r'transition_prob_matrix.csv')\n",
    "\n",
    "print(price_transition_states.head())\n",
    "print(price_transition_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.809830</td>\n",
       "      <td>2.730574</td>\n",
       "      <td>0.770491</td>\n",
       "      <td>2.966650</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.785441</td>\n",
       "      <td>2.453663</td>\n",
       "      <td>0.771422</td>\n",
       "      <td>2.964841</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.823926</td>\n",
       "      <td>2.238301</td>\n",
       "      <td>0.790901</td>\n",
       "      <td>2.914317</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.005571</td>\n",
       "      <td>1.581941</td>\n",
       "      <td>0.918171</td>\n",
       "      <td>2.084867</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.798525</td>\n",
       "      <td>2.693030</td>\n",
       "      <td>0.762935</td>\n",
       "      <td>2.690978</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.096901</td>\n",
       "      <td>2.540894</td>\n",
       "      <td>0.933728</td>\n",
       "      <td>2.026588</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.228067</td>\n",
       "      <td>1.985250</td>\n",
       "      <td>0.941085</td>\n",
       "      <td>2.525047</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.118906</td>\n",
       "      <td>3.728712</td>\n",
       "      <td>1.055625</td>\n",
       "      <td>2.475710</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.847625</td>\n",
       "      <td>2.102348</td>\n",
       "      <td>1.363938</td>\n",
       "      <td>1.771260</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.761153</td>\n",
       "      <td>1.718405</td>\n",
       "      <td>0.852639</td>\n",
       "      <td>1.856784</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          X1        X2        X3        X4  X5   id\n",
       "0   0.809830  2.730574  0.770491  2.966650   0    1\n",
       "1   0.785441  2.453663  0.771422  2.964841   0    2\n",
       "2   0.823926  2.238301  0.790901  2.914317   0    3\n",
       "3   1.005571  1.581941  0.918171  2.084867   0    4\n",
       "4   0.798525  2.693030  0.762935  2.690978   0    5\n",
       "..       ...       ...       ...       ...  ..  ...\n",
       "95  1.096901  2.540894  0.933728  2.026588   0   96\n",
       "96  1.228067  1.985250  0.941085  2.525047   0   97\n",
       "97  1.118906  3.728712  1.055625  2.475710   0   98\n",
       "98  0.847625  2.102348  1.363938  1.771260   0   99\n",
       "99  0.761153  1.718405  0.852639  1.856784   0  100\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_transition_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V91</th>\n",
       "      <th>V92</th>\n",
       "      <th>V93</th>\n",
       "      <th>V94</th>\n",
       "      <th>V95</th>\n",
       "      <th>V96</th>\n",
       "      <th>V97</th>\n",
       "      <th>V98</th>\n",
       "      <th>V99</th>\n",
       "      <th>V100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050283</td>\n",
       "      <td>0.020251</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.026568</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>0.016022</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.006934</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.008172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.023169</td>\n",
       "      <td>0.016224</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.024936</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020574</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.013634</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.009950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034444</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.015208</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>0.019615</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.009703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017591</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.007142</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.011003</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.012562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010919</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.013787</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.015517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042309</td>\n",
       "      <td>0.016379</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.003890</td>\n",
       "      <td>0.027125</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.017501</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016193</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.004994</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.008212</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.008316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.022406</td>\n",
       "      <td>0.010178</td>\n",
       "      <td>0.011546</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0.015755</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.010308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.014125</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>0.012477</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.013129</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.012457</td>\n",
       "      <td>0.014006</td>\n",
       "      <td>0.033324</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.009970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.013230</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>0.014083</td>\n",
       "      <td>0.006911</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.013875</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.008857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>0.012828</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.012384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>0.004460</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.013918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.013686</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>0.007995</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.013291</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.032084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.050283  0.020251  0.017505  0.003655  0.026568  0.006548  0.006352   \n",
       "1   0.040564  0.023169  0.016224  0.003816  0.024936  0.007435  0.007776   \n",
       "2   0.034444  0.016302  0.015208  0.004359  0.022437  0.006756  0.007631   \n",
       "3   0.017591  0.008223  0.009868  0.007142  0.013897  0.006043  0.011003   \n",
       "4   0.042309  0.016379  0.015619  0.003890  0.027125  0.006523  0.006811   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.022406  0.010178  0.011546  0.005559  0.015755  0.005905  0.007434   \n",
       "96  0.014125  0.006274  0.008583  0.005706  0.010867  0.005148  0.006759   \n",
       "97  0.013230  0.006321  0.007653  0.004281  0.010171  0.004477  0.006573   \n",
       "98  0.016398  0.007619  0.009392  0.006894  0.012828  0.006098  0.009410   \n",
       "99  0.013686  0.007597  0.008115  0.007995  0.012092  0.005991  0.017418   \n",
       "\n",
       "          V8        V9       V10  ...       V91       V92       V93       V94  \\\n",
       "0   0.008375  0.016022  0.013590  ...  0.017482  0.003807  0.008912  0.005710   \n",
       "1   0.006751  0.014843  0.012902  ...  0.020574  0.003186  0.010063  0.005734   \n",
       "2   0.008431  0.019615  0.015308  ...  0.016124  0.004657  0.010655  0.004589   \n",
       "3   0.009424  0.022074  0.012562  ...  0.010919  0.007949  0.013787  0.002806   \n",
       "4   0.008304  0.017501  0.014749  ...  0.016193  0.004160  0.009280  0.004994   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.010891  0.027838  0.015147  ...  0.010508  0.008855  0.010696  0.003277   \n",
       "96  0.012477  0.030100  0.013551  ...  0.007031  0.013129  0.010076  0.002364   \n",
       "97  0.022304  0.023152  0.009947  ...  0.007212  0.014083  0.006911  0.002808   \n",
       "98  0.009549  0.022639  0.012384  ...  0.010010  0.010943  0.011931  0.002618   \n",
       "99  0.003487  0.013291  0.008660  ...  0.012523  0.003063  0.016260  0.002462   \n",
       "\n",
       "         V95       V96       V97       V98       V99      V100  \n",
       "0   0.016266  0.008533  0.006934  0.002275  0.001474  0.008172  \n",
       "1   0.013634  0.008189  0.006430  0.001822  0.001520  0.009950  \n",
       "2   0.014261  0.009842  0.009434  0.002486  0.001853  0.009703  \n",
       "3   0.011120  0.011316  0.016923  0.003837  0.003444  0.015517  \n",
       "4   0.017674  0.008824  0.008212  0.002280  0.001601  0.008316  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.013045  0.012874  0.017172  0.004015  0.003121  0.010308  \n",
       "96  0.012457  0.014006  0.033324  0.005777  0.003969  0.009970  \n",
       "97  0.011422  0.010620  0.013875  0.008872  0.003258  0.008857  \n",
       "98  0.011045  0.012419  0.017758  0.004460  0.005623  0.013918  \n",
       "99  0.007017  0.007676  0.009589  0.001575  0.002257  0.032084  \n",
       "\n",
       "[100 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_transition_matrix_np = price_transition_matrix.to_numpy()\n",
    "price_transition_states_np = price_transition_states.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_prices(states, transition, T):\n",
    "    state_indices = np.arange(states.shape[0])\n",
    "\n",
    "    price_simu = np.zeros((T, 6)) #create a matrix to store the simulated prices\n",
    "    price_simu[0] = states[0] #fix the initial vector of prices\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        preceding_state = price_simu[t-1, :] #take the preceding state\n",
    "        index_preceding_state = int(preceding_state[-1] - 1) #take the index of the preceding state (-1 for 0-indexing in Python)\n",
    "        index_next_state = np.random.choice(state_indices, p=(transition[index_preceding_state, :].flatten())) #draw the next state\n",
    "        price_simu[t, :] = states[index_next_state] #update the price vector and store it\n",
    "    return price_simu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "price_50_by_6 = simulate_prices(price_transition_states_np, price_transition_matrix_np, T)\n",
    "prices_50_by_4 = price_50_by_6[:, :-2] #remove the indices column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate Utility data\n",
    "utility_np = np.zeros((1+J, N, T)) # 1 for the outside option, J for the number of products\n",
    "for t in range(T):\n",
    "    for i in range(N):\n",
    "        utility_np[0, i, t] = np.random.gumbel() #outside option, just a random noise\n",
    "        utility_np[1:, i, t] = betas_np[i, :] + etas_np[i]*prices_50_by_4[t, :] + np.random.gumbel(size=J) #utility for the J products\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_jnp = jnp.argmax(utility_np, axis=0) #argmax to get the choice number\n",
    "prices_50_by_4_jnp = jnp.array(prices_50_by_4) #convert the prices to jnp array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I converted most of the numpy objects to jax.numpy objects. JAX is a library that allows us to do parallel computing, which massively speeds up the computation. In addition, it allows us to use relatively easily the optimizer of our choice (I went with Adam), and do automatic differentiation (any function f(x) that is defined, I can just do f_prime = grad(f), and now I have a function that gives me the gradient of f). The only requirement is to write vectorized code (avoid for loops as much as you can). If you don't know how, write the function with for loops, and then ask chatgpt how to make it jit-compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def choice_probas(theta):\n",
    "    theta_jnp = jnp.array(theta)\n",
    "    betas = theta_jnp[:-1]\n",
    "    eta = theta_jnp[-1]\n",
    "    v_1to4_utility = betas + eta * prices_50_by_4_jnp #for a candidate theta, compute systematic utility for each time period and product\n",
    "    v_default = jnp.zeros((T, 1))\n",
    "    v_utility = jnp.concatenate((v_default, v_1to4_utility), axis=1)\n",
    "\n",
    "    # Compute choice probabilities with improved numerical stability\n",
    "    log_sumexps = logsumexp(v_utility, axis=1)\n",
    "    probas = jnp.exp(v_utility - log_sumexps[:, None]) #get the choice probabilities for each time period and product\n",
    "\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood(theta): #(log)-likelihood function\n",
    "    probas_theta = choice_probas(theta) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(T), choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "grad_likelihood = jit(grad(likelihood)) ## gradient of the likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_adam(f, grad_f, x0, norm=1e9, tol=0.1, lr=0.05, maxiter=1000, verbose=0, *args): ## generic adam optimizer\n",
    "  \"\"\"\n",
    "  Generic Adam Optimizer. Specify a function f, a starting point x0, possibly a \\n\n",
    "  learning rate in (0, 1). The lower the learning rate, the more stable (and slow) the convergence.\n",
    "  \"\"\"\n",
    "  tic = time.time()\n",
    "  solver = optax.adam(learning_rate=lr)\n",
    "  params = jnp.array(x0, dtype=jnp.float32)\n",
    "  opt_state = solver.init(params)\n",
    "  iternum = 0\n",
    "  while norm > tol and iternum < maxiter :\n",
    "    iternum += 1\n",
    "    grad = grad_f(params, *args)\n",
    "    updates, opt_state = solver.update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    params = jnp.asarray(params, dtype=jnp.float32)\n",
    "    norm = jnp.max(jnp.abs(grad))\n",
    "    if verbose > 0:\n",
    "      if iternum % 100 == 0:\n",
    "        print(f\"Iteration: {iternum}  Norm: {norm}  theta: {params}\")\n",
    "    if verbose > 1:\n",
    "      print(f\"Iteration: {iternum}  Norm: {norm}  theta: {params}\")\n",
    "  tac = time.time()\n",
    "  if iternum == maxiter:\n",
    "    print(f\"Convergence not reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "  else:\n",
    "    print(f\"Convergence reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 335.4840087890625  theta: [-1.4224386   0.07654052 -1.1986421  -0.5866302  -0.5061392 ]\n",
      "Iteration: 200  Norm: 24.041778564453125  theta: [-1.5150231  -0.15054329 -1.2758065  -0.82709366 -0.40474826]\n",
      "Iteration: 300  Norm: 4.161994934082031  theta: [-1.5439749  -0.22423369 -1.3037015  -0.9119768  -0.37245965]\n",
      "Iteration: 400  Norm: 0.4395751953125  theta: [-1.5494919  -0.23826328 -1.3090049  -0.92812485 -0.3663207 ]\n",
      "Convergence reached after 447 iterations. \n",
      "Time: 5.804388999938965 seconds. Norm: 0.0990447998046875\n"
     ]
    }
   ],
   "source": [
    "theta_MLE_homo = minimize_adam(likelihood, grad_likelihood, jnp.ones(5), lr=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula to retrieve MLE Standard Errors\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla l_{it} &= \\frac{\\partial l_{it} (\\theta)}{\\partial \\theta} \\tag{column vector} \\\\\n",
    "SE(\\widehat{\\theta}) &= diag\\Bigg[{\\sqrt{\\Big(\\sum_{i=1}^N \\sum_{t=1}^T \\nabla l_{it} \\cdot \\nabla l_{it}' \\Big)^{-1}}}\\Bigg]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computation of standard errors\n",
    "\n",
    "@jit\n",
    "def likelihood_it(theta, i, t):\n",
    "    \"\"\"\n",
    "    Computes the likelihood for an individual observation\n",
    "    \"\"\"\n",
    "    probas_theta = choice_probas(theta)\n",
    "    likelihood_it = jnp.log(probas_theta[t, choice_jnp[i, t]])\n",
    "    return likelihood_it\n",
    "\n",
    "grad_likelihood_it = jit(grad(likelihood_it)) ### Takes the gradient of the individual likelihood\n",
    "\n",
    "@jit\n",
    "def outer_grad_likelihood(theta, i, t):\n",
    "    \"\"\"\n",
    "    Takes the outer product (column vector x row vector) of the gradient of the individual likelihood\n",
    "    \"\"\"\n",
    "    grad_it = (grad_likelihood_it(theta, i, t)).reshape(-1, 1) \n",
    "    return grad_it@grad_it.T\n",
    "\n",
    "\n",
    "#computes the outer product above for each individual and time period\n",
    "grad_likelihood_it_vec = vmap(vmap(outer_grad_likelihood, in_axes=(None, 0, None)), in_axes=(None, None, 0)) \n",
    "\n",
    "@jit\n",
    "def compute_standard_errors(theta):\n",
    "    sum_outers = (jnp.sum(grad_likelihood_it_vec(theta, jnp.arange(N), jnp.arange(T)), axis=(0, 1)))\n",
    "    return jnp.diag(jnp.sqrt(jnp.linalg.inv(sum_outers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE assuming homogeneous $\\Theta^h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\Theta^h$: [-1.5499601  -0.23945011 -1.309453   -0.92949164 -0.36580124]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$se$: [0.0242196  0.04772623 0.02251137 0.05515778 0.02070105]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "se = compute_standard_errors(theta_MLE_homo)\n",
    "display(Latex(f'$\\Theta^h$: {theta_MLE_homo}'))\n",
    "display(Latex(f'$se$: {se}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6226189 -0.3826288 -1.3769871 -1.094965  -0.4279044]\n",
      "[-1.4773014  -0.09627143 -1.2419189  -0.7640183  -0.3036981 ]\n"
     ]
    }
   ],
   "source": [
    "###Two classes: instead of estimating theta, we want to estimate the weights phi_1, phi_2 of each class (?)\n",
    "theta_k1 = theta_MLE_homo - 3*se\n",
    "theta_k2 = theta_MLE_homo + 3*se\n",
    "print(theta_k1)\n",
    "print(theta_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def choice_probas_2classes(phi1):\n",
    "    phi2 = 1 - phi1\n",
    "    probas_k1 = choice_probas(theta_k1)\n",
    "    probas_k2 = choice_probas(theta_k2)\n",
    "    probas = phi1 * probas_k1 + phi2 * probas_k2\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood_2classes(phi1): #(log)-likelihood function\n",
    "    probas_theta = choice_probas_2classes(phi1) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(T), choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "\n",
    "grad_likelihood_2classes = jit(grad(likelihood_2classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 47 iterations. \n",
      "Time: 0.9072363376617432 seconds. Norm: 0.0\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_1^h$: [-1.6226189 -0.3826288 -1.3769871 -1.094965  -0.4279044]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_2^h$: [-1.4773014  -0.09627143 -1.2419189  -0.7640183  -0.3036981 ]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (0.5217000246047974, 0.47829997539520264)\n"
     ]
    }
   ],
   "source": [
    "weight_1 = minimize_adam(likelihood_2classes, grad_likelihood_2classes, 0.5, verbose=False)\n",
    "display(Latex(f'$\\Theta_1^h$: {theta_k1}'))\n",
    "display(Latex(f'$\\Theta_2^h$: {theta_k2}'))\n",
    "print(f'Weights: {weight_1.item(), 1-weight_1.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE assuming that $\\Theta^h$ has a normal distribution across the households \n",
    "\n",
    "Prompt, given to French AI 'Le Chat, by Mistral AI':\n",
    "* Yo le chat ! Je voudrais faire maximum likelihood, under assumption that beta is a random coefficient that is normally distributed. HOw do I do this ?\n",
    "* I'll show you my current code for MLE estimation under homogeneous theta. Show me how to modify it to get what I want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of Monte Carlo draws\n",
    "S = 5000  # Number of simulation draws\n",
    "\n",
    "# Generate random draws for Monte Carlo integration\n",
    "key = jax.random.PRNGKey(123)\n",
    "mc_draws = jax.random.normal(key, (S, 5))  # 5 parameters (4 betas + 1 eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood(theta):\n",
    "    mu = theta[:5]  # Mean of the random parameters\n",
    "    sigma = jnp.diag(jnp.exp(theta[5:]))\n",
    "    \n",
    "    betas_eta = mu + jnp.dot(mc_draws, sigma.T)\n",
    "    \n",
    "    def compute_probas(draw):\n",
    "        theta_draw = jnp.concatenate([draw[:-1], draw[-1:]])\n",
    "        return choice_probas(theta_draw)  # Shape: (T, 5)\n",
    "    \n",
    "    probas_theta = vmap(compute_probas)(betas_eta)  # Shape: (S, T, 5)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 5)\n",
    "    \n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(T), choice_jnp]))  # Sum over T\n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood = jit(grad(mixed_logit_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 38.881752014160156  theta: [-1.4626678  -0.03722309 -1.2412455  -0.8680748  -0.6965911  -0.17063989\n",
      "  0.1375886  -0.10701182  0.07265662 -0.4742698 ]\n",
      "Iteration: 200  Norm: 19.294261932373047  theta: [-1.2771003   0.09429025 -1.1077079  -0.77686805 -0.8211678  -0.3026441\n",
      "  0.25629997 -0.1264367   0.19138934 -0.28383934]\n",
      "Iteration: 300  Norm: 12.026185989379883  theta: [-1.1257925   0.19795975 -0.98336947 -0.71182233 -0.9334067  -0.4187171\n",
      "  0.3739883  -0.1566698   0.3030218  -0.1505438 ]\n",
      "Iteration: 400  Norm: 7.024297714233398  theta: [-1.0207092   0.2603968  -0.8879968  -0.6827333  -1.016519   -0.5071937\n",
      "  0.46584672 -0.19185889  0.3906151  -0.06338145]\n",
      "Iteration: 500  Norm: 3.9873409271240234  theta: [-0.9520759   0.29044646 -0.82088774 -0.68220127 -1.0724275  -0.5738261\n",
      "  0.53353536 -0.22704166  0.4582516  -0.0085949 ]\n",
      "Iteration: 600  Norm: 2.2132742404937744  theta: [-0.90825886  0.29828754 -0.7751599  -0.701197   -1.1081074  -0.6263722\n",
      "  0.58419853 -0.26064852  0.51206535  0.0253199 ]\n",
      "Iteration: 700  Norm: 1.2312394380569458  theta: [-0.8801269   0.29188818 -0.7437181  -0.73228586 -1.1304215  -0.6705001\n",
      "  0.62420416 -0.29276475  0.55673724  0.04642684]\n",
      "Iteration: 800  Norm: 0.7727640271186829  theta: [-0.8613701   0.27689663 -0.72109604 -0.7700955  -1.1446551  -0.70971054\n",
      "  0.657903   -0.32386652  0.5953386   0.06004621]\n",
      "Iteration: 900  Norm: 0.7395833730697632  theta: [-0.84797204  0.25712094 -0.7036501  -0.8110566  -1.1543704  -0.7459592\n",
      "  0.6878646  -0.35432875  0.6297501   0.06950486]\n",
      "Iteration: 1000  Norm: 0.682857096195221  theta: [-0.8375634   0.2350377  -0.6891867  -0.8529561  -1.1617168  -0.7802521\n",
      "  0.71537393 -0.38433176  0.66105825  0.07673354]\n",
      "Iteration: 1100  Norm: 0.617247462272644  theta: [-0.82885194  0.2122396  -0.67651343 -0.8945     -1.1678501  -0.8130799\n",
      "  0.7409199  -0.41391912  0.68984306  0.08278143]\n",
      "Iteration: 1200  Norm: 0.5565417408943176  theta: [-0.8211908   0.1897674  -0.6650439  -0.93496007 -1.1733179  -0.8446898\n",
      "  0.7645672  -0.44307515  0.7164163   0.08814422]\n",
      "Iteration: 1300  Norm: 0.4984442889690399  theta: [-0.81426674  0.16833368 -0.6544936  -0.97393304 -1.1783507  -0.8752323\n",
      "  0.7862133  -0.47176108  0.7409563   0.09304988]\n",
      "Iteration: 1400  Norm: 0.44373026490211487  theta: [-0.807938    0.14843161 -0.6447493  -1.0112149  -1.1830052  -0.90482694\n",
      "  0.8057208  -0.49994236  0.7635882   0.0975668 ]\n",
      "Iteration: 1500  Norm: 0.396753191947937  theta: [-0.802121    0.13041799 -0.63573354 -1.0467024  -1.1872916  -0.93359715\n",
      "  0.8229966  -0.5275972   0.7844236   0.10173613]\n",
      "Iteration: 1600  Norm: 0.3498198390007019  theta: [-0.7967722   0.11452582 -0.6274034  -1.0803626  -1.1912024  -0.96167606\n",
      "  0.8380053  -0.5547176   0.80357826  0.10556339]\n",
      "Iteration: 1700  Norm: 0.3124651610851288  theta: [-0.79185426  0.10088385 -0.61972785 -1.1122104  -1.1947244  -0.9892003\n",
      "  0.8507712  -0.5813036   0.82116514  0.10905399]\n",
      "Iteration: 1800  Norm: 0.2764681279659271  theta: [-0.7873289   0.08952188 -0.612655   -1.1422802  -1.1978616  -1.0163108\n",
      "  0.86138105 -0.60736126  0.83730066  0.11222436]\n",
      "Iteration: 1900  Norm: 0.24603679776191711  theta: [-0.78316146  0.08040445 -0.60614914 -1.1706322  -1.200624   -1.0431496\n",
      "  0.8699622  -0.6329003   0.85209984  0.11508675]\n",
      "Iteration: 2000  Norm: 0.21938501298427582  theta: [-0.7793206   0.07340399 -0.600168   -1.1973265  -1.2030323  -1.0698526\n",
      "  0.87668425 -0.6579368   0.86567324  0.11766133]\n",
      "Iteration: 2100  Norm: 0.19024938344955444  theta: [-0.7757782   0.06835091 -0.59467304 -1.2224387  -1.2051036  -1.0965512\n",
      "  0.8817237  -0.6824793   0.87811744  0.11996617]\n",
      "Iteration: 2200  Norm: 0.1728358268737793  theta: [-0.77250534  0.06503974 -0.5896262  -1.2460419  -1.2068658  -1.1233715\n",
      "  0.885274   -0.70654625  0.88953626  0.12201418]\n",
      "Iteration: 2300  Norm: 0.15053801238536835  theta: [-0.7694623   0.0632394  -0.58498234 -1.2681979  -1.2083576  -1.150423\n",
      "  0.8875507  -0.7301451   0.9000171   0.12383819]\n",
      "Iteration: 2400  Norm: 0.12930823862552643  theta: [-0.7666363   0.06270727 -0.58069813 -1.2889647  -1.209616   -1.1778138\n",
      "  0.8887755  -0.7532961   0.9096389   0.12546413]\n",
      "Iteration: 2500  Norm: 0.1295900046825409  theta: [-0.7639975   0.06317314 -0.57675654 -1.3084106  -1.2106487  -1.2056323\n",
      "  0.88911515 -0.77600706  0.9184596   0.12688619]\n",
      "Convergence reached after 2560 iterations. \n",
      "Time: 53.24555444717407 seconds. Norm: 0.09538687020540237\n"
     ]
    }
   ],
   "source": [
    "x_start = jnp.concatenate((theta_MLE_homo, jnp.zeros(5)))\n",
    "\n",
    "theta_mixed_logit = minimize_adam(mixed_logit_likelihood, grad_mixed_logit_likelihood, x_start, lr=0.05, maxiter=5000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.76251066,  0.06384984, -0.5745353 , -1.3194557 , -1.2111914 ],      dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Estimate of Theta \n",
    "theta_mixed_logit[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.29447407, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 2.4326634 , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.45410383, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 2.5178545 , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.1361761 ]],      dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Estimate of the variance-covariance matrix\n",
    "jnp.diag(jnp.exp(theta_mixed_logit[5:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EIO_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "from IPython.display import display, Latex\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.71]\n",
      " [ 0.44]\n",
      " [-1.37]\n",
      " [-0.91]\n",
      " [-1.23]] \n",
      "\n",
      "[[3.22 0.   0.   0.   0.  ]\n",
      " [0.   3.24 0.   0.   0.  ]\n",
      " [0.   0.   2.87 0.   0.  ]\n",
      " [0.   0.   0.   4.15 0.  ]\n",
      " [0.   0.   0.   0.   1.38]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "N = 1000\n",
    "J = 4\n",
    "T = 50\n",
    "\n",
    "# Generate the data\n",
    "np.random.seed(123)\n",
    "mu = np.array([-1.71, 0.44, -1.37, -0.91, -1.23]).reshape(-1, 1)\n",
    "sigma = np.diag(np.array([3.22, 3.24, 2.87, 4.15, 1.38])).reshape(5, 5)\n",
    "\n",
    "# sigma = np.diag(np.zeros(5)).reshape(5, 5) #This was done for testing, and we do recover the true parameters well enough with the MLE homogeneous approach\n",
    "\n",
    "\n",
    "\n",
    "print(mu, '\\n')\n",
    "print(sigma)\n",
    "\n",
    "\n",
    "# generate the random parameters\n",
    "betas = np.random.multivariate_normal(mu.flatten(), sigma, N)\n",
    "betas_np = betas[:, :-1]\n",
    "etas_np = betas[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X1        X2        X3        X4  X5  id\n",
      "0  0.809830  2.730574  0.770491  2.966650   0   1\n",
      "1  0.785441  2.453663  0.771422  2.964841   0   2\n",
      "2  0.823926  2.238301  0.790901  2.914317   0   3\n",
      "3  1.005571  1.581941  0.918171  2.084867   0   4\n",
      "4  0.798525  2.693030  0.762935  2.690978   0   5\n",
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0  0.050283  0.020251  0.017505  0.003655  0.026568  0.006548  0.006352   \n",
      "1  0.040564  0.023169  0.016224  0.003816  0.024936  0.007435  0.007776   \n",
      "2  0.034444  0.016302  0.015208  0.004359  0.022437  0.006756  0.007631   \n",
      "3  0.017591  0.008223  0.009868  0.007142  0.013897  0.006043  0.011003   \n",
      "4  0.042309  0.016379  0.015619  0.003890  0.027125  0.006523  0.006811   \n",
      "\n",
      "         V8        V9       V10  ...       V91       V92       V93       V94  \\\n",
      "0  0.008375  0.016022  0.013590  ...  0.017482  0.003807  0.008912  0.005710   \n",
      "1  0.006751  0.014843  0.012902  ...  0.020574  0.003186  0.010063  0.005734   \n",
      "2  0.008431  0.019615  0.015308  ...  0.016124  0.004657  0.010655  0.004589   \n",
      "3  0.009424  0.022074  0.012562  ...  0.010919  0.007949  0.013787  0.002806   \n",
      "4  0.008304  0.017501  0.014749  ...  0.016193  0.004160  0.009280  0.004994   \n",
      "\n",
      "        V95       V96       V97       V98       V99      V100  \n",
      "0  0.016266  0.008533  0.006934  0.002275  0.001474  0.008172  \n",
      "1  0.013634  0.008189  0.006430  0.001822  0.001520  0.009950  \n",
      "2  0.014261  0.009842  0.009434  0.002486  0.001853  0.009703  \n",
      "3  0.011120  0.011316  0.016923  0.003837  0.003444  0.015517  \n",
      "4  0.017674  0.008824  0.008212  0.002280  0.001601  0.008316  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "###Extract data\n",
    "\n",
    "price_transition_states = pd.read_csv(r'price_transition_states.csv')\n",
    "price_transition_matrix = pd.read_csv(r'transition_prob_matrix.csv')\n",
    "\n",
    "print(price_transition_states.head())\n",
    "print(price_transition_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.809830</td>\n",
       "      <td>2.730574</td>\n",
       "      <td>0.770491</td>\n",
       "      <td>2.966650</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.785441</td>\n",
       "      <td>2.453663</td>\n",
       "      <td>0.771422</td>\n",
       "      <td>2.964841</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.823926</td>\n",
       "      <td>2.238301</td>\n",
       "      <td>0.790901</td>\n",
       "      <td>2.914317</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.005571</td>\n",
       "      <td>1.581941</td>\n",
       "      <td>0.918171</td>\n",
       "      <td>2.084867</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.798525</td>\n",
       "      <td>2.693030</td>\n",
       "      <td>0.762935</td>\n",
       "      <td>2.690978</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.096901</td>\n",
       "      <td>2.540894</td>\n",
       "      <td>0.933728</td>\n",
       "      <td>2.026588</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.228067</td>\n",
       "      <td>1.985250</td>\n",
       "      <td>0.941085</td>\n",
       "      <td>2.525047</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.118906</td>\n",
       "      <td>3.728712</td>\n",
       "      <td>1.055625</td>\n",
       "      <td>2.475710</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.847625</td>\n",
       "      <td>2.102348</td>\n",
       "      <td>1.363938</td>\n",
       "      <td>1.771260</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.761153</td>\n",
       "      <td>1.718405</td>\n",
       "      <td>0.852639</td>\n",
       "      <td>1.856784</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          X1        X2        X3        X4  X5   id\n",
       "0   0.809830  2.730574  0.770491  2.966650   0    1\n",
       "1   0.785441  2.453663  0.771422  2.964841   0    2\n",
       "2   0.823926  2.238301  0.790901  2.914317   0    3\n",
       "3   1.005571  1.581941  0.918171  2.084867   0    4\n",
       "4   0.798525  2.693030  0.762935  2.690978   0    5\n",
       "..       ...       ...       ...       ...  ..  ...\n",
       "95  1.096901  2.540894  0.933728  2.026588   0   96\n",
       "96  1.228067  1.985250  0.941085  2.525047   0   97\n",
       "97  1.118906  3.728712  1.055625  2.475710   0   98\n",
       "98  0.847625  2.102348  1.363938  1.771260   0   99\n",
       "99  0.761153  1.718405  0.852639  1.856784   0  100\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_transition_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V91</th>\n",
       "      <th>V92</th>\n",
       "      <th>V93</th>\n",
       "      <th>V94</th>\n",
       "      <th>V95</th>\n",
       "      <th>V96</th>\n",
       "      <th>V97</th>\n",
       "      <th>V98</th>\n",
       "      <th>V99</th>\n",
       "      <th>V100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050283</td>\n",
       "      <td>0.020251</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.026568</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>0.016022</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.006934</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.008172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.023169</td>\n",
       "      <td>0.016224</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.024936</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020574</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.013634</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.009950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034444</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.015208</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>0.019615</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.009703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017591</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.007142</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.011003</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.012562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010919</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.013787</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.015517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042309</td>\n",
       "      <td>0.016379</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.003890</td>\n",
       "      <td>0.027125</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.017501</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016193</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.004994</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.008212</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.008316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.022406</td>\n",
       "      <td>0.010178</td>\n",
       "      <td>0.011546</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0.015755</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.010308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.014125</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>0.012477</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.013129</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.012457</td>\n",
       "      <td>0.014006</td>\n",
       "      <td>0.033324</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.009970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.013230</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>0.014083</td>\n",
       "      <td>0.006911</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.013875</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.008857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>0.012828</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.012384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>0.004460</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.013918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.013686</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>0.007995</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.013291</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.032084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.050283  0.020251  0.017505  0.003655  0.026568  0.006548  0.006352   \n",
       "1   0.040564  0.023169  0.016224  0.003816  0.024936  0.007435  0.007776   \n",
       "2   0.034444  0.016302  0.015208  0.004359  0.022437  0.006756  0.007631   \n",
       "3   0.017591  0.008223  0.009868  0.007142  0.013897  0.006043  0.011003   \n",
       "4   0.042309  0.016379  0.015619  0.003890  0.027125  0.006523  0.006811   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.022406  0.010178  0.011546  0.005559  0.015755  0.005905  0.007434   \n",
       "96  0.014125  0.006274  0.008583  0.005706  0.010867  0.005148  0.006759   \n",
       "97  0.013230  0.006321  0.007653  0.004281  0.010171  0.004477  0.006573   \n",
       "98  0.016398  0.007619  0.009392  0.006894  0.012828  0.006098  0.009410   \n",
       "99  0.013686  0.007597  0.008115  0.007995  0.012092  0.005991  0.017418   \n",
       "\n",
       "          V8        V9       V10  ...       V91       V92       V93       V94  \\\n",
       "0   0.008375  0.016022  0.013590  ...  0.017482  0.003807  0.008912  0.005710   \n",
       "1   0.006751  0.014843  0.012902  ...  0.020574  0.003186  0.010063  0.005734   \n",
       "2   0.008431  0.019615  0.015308  ...  0.016124  0.004657  0.010655  0.004589   \n",
       "3   0.009424  0.022074  0.012562  ...  0.010919  0.007949  0.013787  0.002806   \n",
       "4   0.008304  0.017501  0.014749  ...  0.016193  0.004160  0.009280  0.004994   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.010891  0.027838  0.015147  ...  0.010508  0.008855  0.010696  0.003277   \n",
       "96  0.012477  0.030100  0.013551  ...  0.007031  0.013129  0.010076  0.002364   \n",
       "97  0.022304  0.023152  0.009947  ...  0.007212  0.014083  0.006911  0.002808   \n",
       "98  0.009549  0.022639  0.012384  ...  0.010010  0.010943  0.011931  0.002618   \n",
       "99  0.003487  0.013291  0.008660  ...  0.012523  0.003063  0.016260  0.002462   \n",
       "\n",
       "         V95       V96       V97       V98       V99      V100  \n",
       "0   0.016266  0.008533  0.006934  0.002275  0.001474  0.008172  \n",
       "1   0.013634  0.008189  0.006430  0.001822  0.001520  0.009950  \n",
       "2   0.014261  0.009842  0.009434  0.002486  0.001853  0.009703  \n",
       "3   0.011120  0.011316  0.016923  0.003837  0.003444  0.015517  \n",
       "4   0.017674  0.008824  0.008212  0.002280  0.001601  0.008316  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.013045  0.012874  0.017172  0.004015  0.003121  0.010308  \n",
       "96  0.012457  0.014006  0.033324  0.005777  0.003969  0.009970  \n",
       "97  0.011422  0.010620  0.013875  0.008872  0.003258  0.008857  \n",
       "98  0.011045  0.012419  0.017758  0.004460  0.005623  0.013918  \n",
       "99  0.007017  0.007676  0.009589  0.001575  0.002257  0.032084  \n",
       "\n",
       "[100 rows x 100 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_transition_matrix_np = price_transition_matrix.to_numpy()\n",
    "price_transition_states_np = price_transition_states.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_prices(states, transition, T):\n",
    "    state_indices = np.arange(states.shape[0])\n",
    "\n",
    "    price_simu = np.zeros((T, 6)) #create a matrix to store the simulated prices\n",
    "    price_simu[0] = states[0] #fix the initial vector of prices\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        preceding_state = price_simu[t-1, :] #take the preceding state\n",
    "        index_preceding_state = int(preceding_state[-1] - 1) #take the index of the preceding state (-1 for 0-indexing in Python)\n",
    "        index_next_state = np.random.choice(state_indices, p=(transition[index_preceding_state, :].flatten())) #draw the next state\n",
    "        price_simu[t, :] = states[index_next_state] #update the price vector and store it\n",
    "    return price_simu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "price_50_by_6 = simulate_prices(price_transition_states_np, price_transition_matrix_np, T)\n",
    "prices_50_by_4 = price_50_by_6[:, :-2] #remove the indices column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate Utility data\n",
    "utility_np = np.zeros((1+J, N, T)) # 1 for the outside option, J for the number of products\n",
    "for t in range(T):\n",
    "    for i in range(N):\n",
    "        utility_np[0, i, t] = np.random.gumbel() #outside option, just a random noise\n",
    "        utility_np[1:, i, t] = betas_np[i, :] + etas_np[i]*prices_50_by_4[t, :] + np.random.gumbel(size=J) #utility for the J products\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_jnp = jnp.argmax(utility_np, axis=0) #argmax to get the choice number\n",
    "prices_50_by_4_jnp = jnp.array(prices_50_by_4) #convert the prices to jnp array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I converted most of the numpy objects to jax.numpy objects. JAX is a library that allows us to do parallel computing, which speeds up the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLE assuming homogeneous $\\Theta^h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def choice_probas(theta):\n",
    "    theta_jnp = jnp.array(theta)\n",
    "    betas = theta_jnp[:-1]\n",
    "    eta = theta_jnp[-1]\n",
    "    v_1to4_utility = betas + eta * prices_50_by_4_jnp #for a candidate theta, compute systematic utility for each time period and product\n",
    "    v_default = jnp.zeros((T, 1))\n",
    "    v_utility = jnp.concatenate((v_default, v_1to4_utility), axis=1)\n",
    "\n",
    "    # Compute choice probabilities with improved numerical stability\n",
    "    log_sumexps = logsumexp(v_utility, axis=1)\n",
    "    probas = jnp.exp(v_utility - log_sumexps[:, None]) #get the choice probabilities for each time period and product\n",
    "\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood(theta): #(log)-likelihood function\n",
    "    probas_theta = choice_probas(theta) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(T), choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "grad_likelihood = jit(grad(likelihood)) ## gradient of the likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_adam(f, grad_f, x0, norm=1e9, tol=0.1, lr=0.05, maxiter=1000, verbose=0, *args): ## generic adam optimizer\n",
    "  \"\"\"\n",
    "  Generic Adam Optimizer. Specify a function f, a starting point x0, possibly a \\n\n",
    "  learning rate in (0, 1). The lower the learning rate, the more stable (and slow) the convergence.\n",
    "  \"\"\"\n",
    "  tic = time.time()\n",
    "  solver = optax.adam(learning_rate=lr)\n",
    "  params = jnp.array(x0, dtype=jnp.float32)\n",
    "  opt_state = solver.init(params)\n",
    "  iternum = 0\n",
    "  while norm > tol and iternum < maxiter :\n",
    "    iternum += 1\n",
    "    grad = grad_f(params, *args)\n",
    "    updates, opt_state = solver.update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    params = jnp.asarray(params, dtype=jnp.float32)\n",
    "    norm = jnp.max(jnp.abs(grad))\n",
    "    if verbose > 0:\n",
    "      if iternum % 100 == 0:\n",
    "        print(f\"Iteration: {iternum}  Norm: {norm}  theta: {params}\")\n",
    "    if verbose > 1:\n",
    "      print(f\"Iteration: {iternum}  Norm: {norm}  theta: {params}\")\n",
    "  tac = time.time()\n",
    "  if iternum == maxiter:\n",
    "    print(f\"Convergence not reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "  else:\n",
    "    print(f\"Convergence reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 151.35691833496094  theta: [-1.3924706   0.14809617 -1.1837898  -0.5084201  -0.5308946 ]\n",
      "Iteration: 200  Norm: 64.03196716308594  theta: [-1.4601713  -0.01200273 -1.2231857  -0.6660787  -0.46587494]\n",
      "Iteration: 300  Norm: 34.01972961425781  theta: [-1.5036387  -0.12207926 -1.2650094  -0.7935174  -0.41740423]\n",
      "Iteration: 400  Norm: 15.053604125976562  theta: [-1.5295694  -0.18775776 -1.2898902  -0.86957914 -0.388516  ]\n",
      "Iteration: 500  Norm: 5.780517578125  theta: [-1.5423031  -0.22003625 -1.302108   -0.90698284 -0.3743311 ]\n",
      "Iteration: 600  Norm: 1.8632659912109375  theta: [-1.5475703  -0.23339154 -1.307161   -0.92246413 -0.36846307]\n",
      "Iteration: 700  Norm: 0.5316925048828125  theta: [-1.5494206  -0.23808509 -1.3089368  -0.92790514 -0.366402  ]\n",
      "Iteration: 800  Norm: 0.141448974609375  theta: [-1.5499736  -0.2394883  -1.3094672  -0.92953134 -0.36578575]\n",
      "Iteration: 900  Norm: 0.02227783203125  theta: [-1.5501127  -0.23984279 -1.3096005  -0.9299428  -0.3656294 ]\n",
      "Convergence reached after 944 iterations. \n",
      "Time: 11.552695751190186 seconds. Norm: 0.0095062255859375\n"
     ]
    }
   ],
   "source": [
    "theta_MLE_homo = minimize_adam(likelihood, grad_likelihood, jnp.ones(5), lr=0.1, verbose=1, tol=0.01, maxiter=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.550132   -0.23989092 -1.3096188  -0.9299984  -0.36560854]\n"
     ]
    }
   ],
   "source": [
    "print(theta_MLE_homo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula to retrieve MLE Standard Errors\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla l_{it} &= \\frac{\\partial l_{it} (\\widehat{\\theta})}{\\partial \\widehat{\\theta}} \\tag{column vector} \\\\\n",
    "SE(\\widehat{\\theta}) &= diag\\Bigg[{\\sqrt{\\Big( \\frac{1}{NT}\\sum_{i=1}^N \\sum_{t=1}^T \\nabla l_{it} \\cdot \\nabla l_{it}' \\Big)^{-1}}}\\Bigg]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computation of standard errors\n",
    "\n",
    "@jit\n",
    "def likelihood_it(theta, i, t):\n",
    "    \"\"\"\n",
    "    Computes the likelihood for an individual observation\n",
    "    \"\"\"\n",
    "    probas_theta = choice_probas(theta)\n",
    "    likelihood_it = jnp.log(probas_theta[t, choice_jnp[i, t]])\n",
    "    return likelihood_it\n",
    "\n",
    "grad_likelihood_it = jit(grad(likelihood_it)) ### Takes the gradient of the individual likelihood\n",
    "\n",
    "@jit\n",
    "def outer_grad_likelihood(theta, i, t):\n",
    "    \"\"\"\n",
    "    Takes the outer product (column vector x row vector) of the gradient of the individual likelihood\n",
    "    \"\"\"\n",
    "    grad_it = (grad_likelihood_it(theta, i, t)).reshape(-1, 1) \n",
    "    return grad_it@grad_it.T\n",
    "\n",
    "\n",
    "#computes the outer product above for each individual and time period\n",
    "grad_likelihood_it_vec = vmap(vmap(outer_grad_likelihood, in_axes=(None, 0, None)), in_axes=(None, None, 0)) \n",
    "\n",
    "@jit\n",
    "def compute_standard_errors(theta):\n",
    "    sum_outers = (1/(N*T))*(jnp.sum(grad_likelihood_it_vec(theta, jnp.arange(N), jnp.arange(T)), axis=(0, 1)))\n",
    "    return jnp.diag(jnp.sqrt(jnp.linalg.inv(sum_outers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\Theta^h$: [-1.550132   -0.23989092 -1.3096188  -0.9299984  -0.36560854]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$se$: [ 5.4158187 10.672416   5.0338407 12.334234   4.629117 ]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "se = compute_standard_errors(theta_MLE_homo)\n",
    "display(Latex(f'$\\Theta^h$: {theta_MLE_homo}'))\n",
    "display(Latex(f'$se$: {se}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLE assuming two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -6.965951  -10.912307   -6.3434596 -13.264233   -4.9947257]\n",
      "[ 3.8656867 10.432525   3.7242217 11.404236   4.2635083]\n"
     ]
    }
   ],
   "source": [
    "###Two classes: instead of estimating theta, we want to estimate the weights phi_1, phi_2 of each class (?)\n",
    "theta_k1 = theta_MLE_homo - se\n",
    "theta_k2 = theta_MLE_homo + se\n",
    "print(theta_k1)\n",
    "print(theta_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def choice_probas_2classes(phi1):\n",
    "    phi2 = 1 - phi1\n",
    "    probas_k1 = choice_probas(theta_k1)\n",
    "    probas_k2 = choice_probas(theta_k2)\n",
    "    probas = phi1 * probas_k1 + phi2 * probas_k2\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood_2classes(phi1): #(log)-likelihood function\n",
    "    probas_theta = choice_probas_2classes(phi1) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(T), choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "\n",
    "grad_likelihood_2classes = jit(grad(likelihood_2classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 165 iterations. \n",
      "Time: 2.404602527618408 seconds. Norm: 0.0703125\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_1^h$: [ -6.965951  -10.912307   -6.3434596 -13.264233   -4.9947257]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_2^h$: [ 3.8656867 10.432525   3.7242217 11.404236   4.2635083]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (0.725957453250885, 0.274042546749115)\n"
     ]
    }
   ],
   "source": [
    "weight_1 = minimize_adam(likelihood_2classes, grad_likelihood_2classes, 0.5, verbose=False)\n",
    "display(Latex(f'$\\Theta_1^h$: {theta_k1}'))\n",
    "display(Latex(f'$\\Theta_2^h$: {theta_k2}'))\n",
    "print(f'Weights: {weight_1.item(), 1-weight_1.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLE assuming that $\\Theta^h$ has a normal distribution across the households "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of Monte Carlo draws\n",
    "S = 5000  # Number of simulation draws\n",
    "\n",
    "# Generate random draws for Monte Carlo integration\n",
    "key = jax.random.PRNGKey(123)\n",
    "mc_draws = jax.random.normal(key, (S, 5))  # 5 parameters (4 betas + 1 eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood(theta):\n",
    "    mu = theta[:5]  # Mean of the random parameters\n",
    "    sigma = jnp.exp(theta[5:])\n",
    "    betas_eta = mu + mc_draws * jnp.sqrt(sigma)\n",
    "    \n",
    "    probas_theta = vmap(choice_probas)(betas_eta)  # Shape: (S, T, 5)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 5)\n",
    "    \n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(T), choice_jnp]))  # Sum over T\n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood = jit(grad(mixed_logit_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 14.214195251464844  theta: [-1.5548645  -0.02482525 -1.3207934  -0.99813336 -1.0442771   0.70649076\n",
      "  1.551473    0.80982864  1.3152521  -0.09989887]\n",
      "Iteration: 200  Norm: 4.883601665496826  theta: [-1.364749    0.0413483  -1.145643   -0.9625387  -1.1118015   0.48166847\n",
      "  1.6117646   0.61018     1.3661939   0.05143078]\n",
      "Iteration: 300  Norm: 2.6269466876983643  theta: [-1.2292743   0.08922753 -1.0077752  -0.93832475 -1.1660131   0.30134913\n",
      "  1.66933     0.4228158   1.4257964   0.15263534]\n",
      "Iteration: 400  Norm: 1.1853811740875244  theta: [-1.1455742   0.11061405 -0.9148244  -0.9353715  -1.1994507   0.1719849\n",
      "  1.7142999   0.2672374   1.4741052   0.21238129]\n",
      "Iteration: 500  Norm: 0.49642378091812134  theta: [-1.0949494   0.11227275 -0.8538697  -0.9485005  -1.216657    0.07611551\n",
      "  1.7485735   0.13745132  1.5133996   0.24276757]\n",
      "Iteration: 600  Norm: 0.41016486287117004  theta: [-1.062452    0.10262876 -0.811927   -0.9712046  -1.2241095  -0.00178099\n",
      "  1.7757965   0.02368816  1.5471905   0.25626093]\n",
      "Iteration: 700  Norm: 0.4041597545146942  theta: [-1.0390031   0.08810835 -0.78051114 -0.9985731  -1.2267646  -0.07090499\n",
      "  1.7986401  -0.08067632  1.5780548   0.26160553]\n",
      "Iteration: 800  Norm: 0.3753504753112793  theta: [-1.0199312   0.07262439 -0.7550231  -1.0276742  -1.2275225  -0.13555403\n",
      "  1.8184147  -0.17893086  1.607382    0.2637769 ]\n",
      "Iteration: 900  Norm: 0.34549716114997864  theta: [-1.0031843   0.05821935 -0.73324656 -1.0570296  -1.2277453  -0.19727771\n",
      "  1.835504   -0.2723881   1.6357161   0.26510662]\n",
      "Iteration: 1000  Norm: 0.30800217390060425  theta: [-0.98798895  0.04582462 -0.7141543  -1.0859797  -1.2279301  -0.2564519\n",
      "  1.8498714  -0.36151883  1.6631536   0.26647606]\n",
      "Iteration: 1100  Norm: 0.278755784034729  theta: [-0.97405505  0.03580514 -0.69723296 -1.1142507  -1.22819    -0.31310284\n",
      "  1.8613712  -0.4465501   1.6896374   0.26808822]\n",
      "Iteration: 1200  Norm: 0.2492464780807495  theta: [-0.9612725   0.02824103 -0.68217397 -1.1417326  -1.228497   -0.3672622\n",
      "  1.8699111  -0.5276859   1.7150779   0.26988542]\n",
      "Iteration: 1300  Norm: 0.22845721244812012  theta: [-0.9495187   0.02307302 -0.66872346 -1.168373   -1.2288108  -0.41904712\n",
      "  1.875525   -0.6051699   1.7394      0.27181175]\n",
      "Iteration: 1400  Norm: 0.2037682682275772  theta: [-0.93872994  0.02012586 -0.65670824 -1.1941504  -1.2290667  -0.46865153\n",
      "  1.8783411  -0.67925304  1.7625574   0.27374354]\n",
      "Convergence reached after 1418 iterations. \n",
      "Time: 30.489825010299683 seconds. Norm: 0.19866441190242767\n"
     ]
    }
   ],
   "source": [
    "x_start = jnp.concatenate((theta_MLE_homo, jnp.ones(5)))\n",
    "theta_mixed_logit = minimize_adam(mixed_logit_likelihood, grad_mixed_logit_likelihood, x_start, lr=0.1, maxiter=5000, verbose=1, tol=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.9368873 ,  0.01981145, -0.6546789 , -1.1986973 , -1.2291056 ],      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Estimate of Theta: not too bad, and the signs are correct\n",
    "theta_mixed_logit[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.6204143, 0.       , 0.       , 0.       , 0.       ],\n",
       "       [0.       , 6.54409  , 0.       , 0.       , 0.       ],\n",
       "       [0.       , 0.       , 0.5004512, 0.       , 0.       ],\n",
       "       [0.       , 0.       , 0.       , 5.850934 , 0.       ],\n",
       "       [0.       , 0.       , 0.       , 0.       , 1.3153167]],      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Estimate of the variance-covariance matrix. Seems hard to recover, probably also causing a larger biase in the point estimate.\n",
    "jnp.diag(jnp.exp(theta_mixed_logit[5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sanity Check: what if we assume the correct sigma and only estimate for theta ?\n",
    "\n",
    "Conclusion: we do recover an unbiased estimate of $\\mu$ if we are able to assume to know $\\sigma$ already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mixed_logit_likelihood_correct(theta):\n",
    "    mu = theta.flatten()  # Mean of the random parameters\n",
    "    betas_eta = mu + mc_draws * jnp.sqrt(jnp.diag(sigma)) \n",
    "\n",
    "    def compute_probas(draw):\n",
    "        return choice_probas(draw)  # Shape: (T, 5)\n",
    "    \n",
    "    probas_theta = vmap(compute_probas)(betas_eta)  # Shape: (S, T, 5)\n",
    "    probas_theta_avg = jnp.mean(probas_theta, axis=0)  # Shape: (T, 5)\n",
    "    \n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta_avg[jnp.arange(T), choice_jnp]))  # Sum over T\n",
    "    return -log_likelihood\n",
    "\n",
    "grad_mixed_logit_likelihood_correct = jit(grad(mixed_logit_likelihood_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 54.19804000854492  theta: [-1.8604772  -0.00691507 -1.4520905  -1.3856395  -1.0146875 ]\n",
      "Iteration: 200  Norm: 21.64241600036621  theta: [-1.7583201   0.26995337 -1.3512261  -1.0736028  -1.1389674 ]\n",
      "Iteration: 300  Norm: 7.9752068519592285  theta: [-1.696638    0.42954588 -1.2925091  -0.88821095 -1.2112354 ]\n",
      "Iteration: 400  Norm: 2.3028757572174072  theta: [-1.670866    0.49620482 -1.2679734  -0.8107995  -1.2414314 ]\n",
      "Iteration: 500  Norm: 0.535210132598877  theta: [-1.6627408  0.517212  -1.2602401 -0.7864084 -1.250948 ]\n",
      "Iteration: 600  Norm: 0.09628894925117493  theta: [-1.6607764   0.52229446 -1.2583694  -0.78050625 -1.2532526 ]\n",
      "Convergence reached after 600 iterations. \n",
      "Time: 12.209501504898071 seconds. Norm: 0.09628894925117493\n"
     ]
    }
   ],
   "source": [
    "theta_mixed_logit_correct = minimize_adam(mixed_logit_likelihood_correct, \n",
    "                                          grad_mixed_logit_likelihood_correct, \n",
    "                                          theta_MLE_homo, \n",
    "                                          lr=0.05, maxiter=5000, verbose=1, tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.6607764 ,  0.52229446, -1.2583694 , -0.78050625, -1.2532526 ],      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_mixed_logit_correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EIO_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

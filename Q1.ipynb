{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "from IPython.display import display, Latex\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.71]\n",
      " [ 0.44]\n",
      " [-1.37]\n",
      " [-0.91]\n",
      " [-1.23]] \n",
      "\n",
      "[[3.22 0.   0.   0.   0.  ]\n",
      " [0.   3.24 0.   0.   0.  ]\n",
      " [0.   0.   2.87 0.   0.  ]\n",
      " [0.   0.   0.   4.15 0.  ]\n",
      " [0.   0.   0.   0.   1.38]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "N = 1000\n",
    "J = 4\n",
    "T = 50\n",
    "\n",
    "# Generate the data\n",
    "np.random.seed(123)\n",
    "mu = np.array([-1.71, 0.44, -1.37, -0.91, -1.23]).reshape(-1, 1)\n",
    "sigma = np.diag(np.array([3.22, 3.24, 2.87, 4.15, 1.38])).reshape(5, 5)\n",
    "\n",
    "print(mu, '\\n')\n",
    "print(sigma)\n",
    "\n",
    "\n",
    "# generate the random parameters\n",
    "betas = np.random.multivariate_normal(mu.flatten(), sigma, N)\n",
    "betas_np = betas[:, :-1]\n",
    "etas_np = betas[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X1        X2        X3        X4  X5  id\n",
      "0  0.809830  2.730574  0.770491  2.966650   0   1\n",
      "1  0.785441  2.453663  0.771422  2.964841   0   2\n",
      "2  0.823926  2.238301  0.790901  2.914317   0   3\n",
      "3  1.005571  1.581941  0.918171  2.084867   0   4\n",
      "4  0.798525  2.693030  0.762935  2.690978   0   5\n",
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0  0.050283  0.020251  0.017505  0.003655  0.026568  0.006548  0.006352   \n",
      "1  0.040564  0.023169  0.016224  0.003816  0.024936  0.007435  0.007776   \n",
      "2  0.034444  0.016302  0.015208  0.004359  0.022437  0.006756  0.007631   \n",
      "3  0.017591  0.008223  0.009868  0.007142  0.013897  0.006043  0.011003   \n",
      "4  0.042309  0.016379  0.015619  0.003890  0.027125  0.006523  0.006811   \n",
      "\n",
      "         V8        V9       V10  ...       V91       V92       V93       V94  \\\n",
      "0  0.008375  0.016022  0.013590  ...  0.017482  0.003807  0.008912  0.005710   \n",
      "1  0.006751  0.014843  0.012902  ...  0.020574  0.003186  0.010063  0.005734   \n",
      "2  0.008431  0.019615  0.015308  ...  0.016124  0.004657  0.010655  0.004589   \n",
      "3  0.009424  0.022074  0.012562  ...  0.010919  0.007949  0.013787  0.002806   \n",
      "4  0.008304  0.017501  0.014749  ...  0.016193  0.004160  0.009280  0.004994   \n",
      "\n",
      "        V95       V96       V97       V98       V99      V100  \n",
      "0  0.016266  0.008533  0.006934  0.002275  0.001474  0.008172  \n",
      "1  0.013634  0.008189  0.006430  0.001822  0.001520  0.009950  \n",
      "2  0.014261  0.009842  0.009434  0.002486  0.001853  0.009703  \n",
      "3  0.011120  0.011316  0.016923  0.003837  0.003444  0.015517  \n",
      "4  0.017674  0.008824  0.008212  0.002280  0.001601  0.008316  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "###Extract data\n",
    "\n",
    "price_transition_states = pd.read_csv(r'price_transition_states.csv')\n",
    "price_transition_matrix = pd.read_csv(r'transition_prob_matrix.csv')\n",
    "\n",
    "print(price_transition_states.head())\n",
    "print(price_transition_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.809830</td>\n",
       "      <td>2.730574</td>\n",
       "      <td>0.770491</td>\n",
       "      <td>2.966650</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.785441</td>\n",
       "      <td>2.453663</td>\n",
       "      <td>0.771422</td>\n",
       "      <td>2.964841</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.823926</td>\n",
       "      <td>2.238301</td>\n",
       "      <td>0.790901</td>\n",
       "      <td>2.914317</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.005571</td>\n",
       "      <td>1.581941</td>\n",
       "      <td>0.918171</td>\n",
       "      <td>2.084867</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.798525</td>\n",
       "      <td>2.693030</td>\n",
       "      <td>0.762935</td>\n",
       "      <td>2.690978</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.096901</td>\n",
       "      <td>2.540894</td>\n",
       "      <td>0.933728</td>\n",
       "      <td>2.026588</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.228067</td>\n",
       "      <td>1.985250</td>\n",
       "      <td>0.941085</td>\n",
       "      <td>2.525047</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.118906</td>\n",
       "      <td>3.728712</td>\n",
       "      <td>1.055625</td>\n",
       "      <td>2.475710</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.847625</td>\n",
       "      <td>2.102348</td>\n",
       "      <td>1.363938</td>\n",
       "      <td>1.771260</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.761153</td>\n",
       "      <td>1.718405</td>\n",
       "      <td>0.852639</td>\n",
       "      <td>1.856784</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          X1        X2        X3        X4  X5   id\n",
       "0   0.809830  2.730574  0.770491  2.966650   0    1\n",
       "1   0.785441  2.453663  0.771422  2.964841   0    2\n",
       "2   0.823926  2.238301  0.790901  2.914317   0    3\n",
       "3   1.005571  1.581941  0.918171  2.084867   0    4\n",
       "4   0.798525  2.693030  0.762935  2.690978   0    5\n",
       "..       ...       ...       ...       ...  ..  ...\n",
       "95  1.096901  2.540894  0.933728  2.026588   0   96\n",
       "96  1.228067  1.985250  0.941085  2.525047   0   97\n",
       "97  1.118906  3.728712  1.055625  2.475710   0   98\n",
       "98  0.847625  2.102348  1.363938  1.771260   0   99\n",
       "99  0.761153  1.718405  0.852639  1.856784   0  100\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_transition_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V91</th>\n",
       "      <th>V92</th>\n",
       "      <th>V93</th>\n",
       "      <th>V94</th>\n",
       "      <th>V95</th>\n",
       "      <th>V96</th>\n",
       "      <th>V97</th>\n",
       "      <th>V98</th>\n",
       "      <th>V99</th>\n",
       "      <th>V100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050283</td>\n",
       "      <td>0.020251</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.026568</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>0.016022</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.006934</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.008172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.023169</td>\n",
       "      <td>0.016224</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.024936</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020574</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.013634</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.009950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034444</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.015208</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>0.019615</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.009703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017591</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.007142</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.011003</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.012562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010919</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.013787</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.015517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042309</td>\n",
       "      <td>0.016379</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.003890</td>\n",
       "      <td>0.027125</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.017501</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016193</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.004994</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.008212</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.008316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.022406</td>\n",
       "      <td>0.010178</td>\n",
       "      <td>0.011546</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0.015755</td>\n",
       "      <td>0.005905</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.010308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.014125</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>0.012477</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.013129</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.012457</td>\n",
       "      <td>0.014006</td>\n",
       "      <td>0.033324</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.009970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.013230</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>0.014083</td>\n",
       "      <td>0.006911</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.013875</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.008857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.016398</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>0.012828</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.012384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>0.004460</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.013918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.013686</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>0.007995</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.013291</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.032084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.050283  0.020251  0.017505  0.003655  0.026568  0.006548  0.006352   \n",
       "1   0.040564  0.023169  0.016224  0.003816  0.024936  0.007435  0.007776   \n",
       "2   0.034444  0.016302  0.015208  0.004359  0.022437  0.006756  0.007631   \n",
       "3   0.017591  0.008223  0.009868  0.007142  0.013897  0.006043  0.011003   \n",
       "4   0.042309  0.016379  0.015619  0.003890  0.027125  0.006523  0.006811   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.022406  0.010178  0.011546  0.005559  0.015755  0.005905  0.007434   \n",
       "96  0.014125  0.006274  0.008583  0.005706  0.010867  0.005148  0.006759   \n",
       "97  0.013230  0.006321  0.007653  0.004281  0.010171  0.004477  0.006573   \n",
       "98  0.016398  0.007619  0.009392  0.006894  0.012828  0.006098  0.009410   \n",
       "99  0.013686  0.007597  0.008115  0.007995  0.012092  0.005991  0.017418   \n",
       "\n",
       "          V8        V9       V10  ...       V91       V92       V93       V94  \\\n",
       "0   0.008375  0.016022  0.013590  ...  0.017482  0.003807  0.008912  0.005710   \n",
       "1   0.006751  0.014843  0.012902  ...  0.020574  0.003186  0.010063  0.005734   \n",
       "2   0.008431  0.019615  0.015308  ...  0.016124  0.004657  0.010655  0.004589   \n",
       "3   0.009424  0.022074  0.012562  ...  0.010919  0.007949  0.013787  0.002806   \n",
       "4   0.008304  0.017501  0.014749  ...  0.016193  0.004160  0.009280  0.004994   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.010891  0.027838  0.015147  ...  0.010508  0.008855  0.010696  0.003277   \n",
       "96  0.012477  0.030100  0.013551  ...  0.007031  0.013129  0.010076  0.002364   \n",
       "97  0.022304  0.023152  0.009947  ...  0.007212  0.014083  0.006911  0.002808   \n",
       "98  0.009549  0.022639  0.012384  ...  0.010010  0.010943  0.011931  0.002618   \n",
       "99  0.003487  0.013291  0.008660  ...  0.012523  0.003063  0.016260  0.002462   \n",
       "\n",
       "         V95       V96       V97       V98       V99      V100  \n",
       "0   0.016266  0.008533  0.006934  0.002275  0.001474  0.008172  \n",
       "1   0.013634  0.008189  0.006430  0.001822  0.001520  0.009950  \n",
       "2   0.014261  0.009842  0.009434  0.002486  0.001853  0.009703  \n",
       "3   0.011120  0.011316  0.016923  0.003837  0.003444  0.015517  \n",
       "4   0.017674  0.008824  0.008212  0.002280  0.001601  0.008316  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.013045  0.012874  0.017172  0.004015  0.003121  0.010308  \n",
       "96  0.012457  0.014006  0.033324  0.005777  0.003969  0.009970  \n",
       "97  0.011422  0.010620  0.013875  0.008872  0.003258  0.008857  \n",
       "98  0.011045  0.012419  0.017758  0.004460  0.005623  0.013918  \n",
       "99  0.007017  0.007676  0.009589  0.001575  0.002257  0.032084  \n",
       "\n",
       "[100 rows x 100 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_transition_matrix_np = price_transition_matrix.to_numpy()\n",
    "price_transition_states_np = price_transition_states.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_prices(states, transition, T):\n",
    "    state_indices = np.arange(states.shape[0])\n",
    "\n",
    "    price_simu = np.zeros((T, 6)) #create a matrix to store the simulated prices\n",
    "    price_simu[0] = states[0] #fix the initial vector of prices\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        preceding_state = price_simu[t-1, :] #take the preceding state\n",
    "        index_preceding_state = int(preceding_state[-1] - 1) #take the index of the preceding state (-1 for 0-indexing in Python)\n",
    "        index_next_state = np.random.choice(state_indices, p=(transition[index_preceding_state, :].flatten())) #draw the next state\n",
    "        price_simu[t, :] = states[index_next_state] #update the price vector and store it\n",
    "    return price_simu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "price_50_by_6 = simulate_prices(price_transition_states_np, price_transition_matrix_np, T)\n",
    "prices_50_by_4 = price_50_by_6[:, :-2] #remove the indices column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate Utility data\n",
    "utility_np = np.zeros((1+J, N, T)) # 1 for the outside option, J for the number of products\n",
    "for t in range(T):\n",
    "    for i in range(N):\n",
    "        utility_np[0, i, t] = np.random.gumbel() #outside option, just a random noise\n",
    "        utility_np[1:, i, t] = betas_np[i, :] + etas_np[i]*prices_50_by_4[t, :] + np.random.gumbel(size=J) #utility for the J products\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_jnp = jnp.argmax(utility_np, axis=0) #argmax to get the choice number\n",
    "prices_50_by_4_jnp = jnp.array(prices_50_by_4) #convert the prices to jnp array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I converted most of the numpy objects to jax.numpy objects. JAX is a library that allows us to do parallel computing, which massively speeds up the computation. In addition, it allows us to use relatively easily the optimizer of our choice (I went with Adam), and do automatic differentiation (any function f(x) that is defined, I can just do f_prime = grad(f), and now I have a function that gives me the gradient of f). The only requirement is to write vectorized code (avoid for loops as much as you can). If you don't know how, write the function with for loops, and then ask chatgpt how to make it jit-compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def choice_probas(theta):\n",
    "    theta_jnp = jnp.array(theta)\n",
    "    betas = theta_jnp[:-1]\n",
    "    eta = theta_jnp[-1]\n",
    "    v_1to4_utility = betas + eta * prices_50_by_4_jnp #for a candidate theta, compute systematic utility for each time period and product\n",
    "    v_default = jnp.zeros((T, 1))\n",
    "    v_utility = jnp.concatenate((v_default, v_1to4_utility), axis=1)\n",
    "\n",
    "    # Compute choice probabilities with improved numerical stability\n",
    "    log_sumexps = logsumexp(v_utility, axis=1)\n",
    "    probas = jnp.exp(v_utility - log_sumexps[:, None]) #get the choice probabilities for each time period and product\n",
    "\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood(theta): #(log)-likelihood function\n",
    "    probas_theta = choice_probas(theta) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(T), choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "grad_likelihood = jit(grad(likelihood)) ## gradient of the likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_adam(f, grad_f, x0, norm=1e9, tol=0.1, lr=0.05, maxiter=1000, verbose=0, *args): ## generic adam optimizer\n",
    "  \"\"\"\n",
    "  Generic Adam Optimizer. Specify a function f, a starting point x0, possibly a \\n\n",
    "  learning rate in (0, 1). The lower the learning rate, the more stable (and slow) the convergence.\n",
    "  \"\"\"\n",
    "  tic = time.time()\n",
    "  solver = optax.adam(learning_rate=lr)\n",
    "  params = jnp.array(x0, dtype=jnp.float32)\n",
    "  opt_state = solver.init(params)\n",
    "  iternum = 0\n",
    "  while norm > tol and iternum < maxiter :\n",
    "    iternum += 1\n",
    "    grad = grad_f(params, *args)\n",
    "    updates, opt_state = solver.update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    params = jnp.asarray(params, dtype=jnp.float32)\n",
    "    norm = jnp.max(jnp.abs(grad))\n",
    "    if verbose > 0:\n",
    "      if iternum % 100 == 0:\n",
    "        print(f\"Iteration: {iternum}  Norm: {norm}  theta: {params}\")\n",
    "    if verbose > 1:\n",
    "      print(f\"Iteration: {iternum}  Norm: {norm}  theta: {params}\")\n",
    "  tac = time.time()\n",
    "  if iternum == maxiter:\n",
    "    print(f\"Convergence not reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "  else:\n",
    "    print(f\"Convergence reached after {iternum} iterations. \\nTime: {tac-tic} seconds. Norm: {norm}\")\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 335.4840087890625  theta: [-1.4224386   0.07654052 -1.1986421  -0.5866302  -0.5061392 ]\n",
      "Iteration: 200  Norm: 24.041778564453125  theta: [-1.5150231  -0.15054329 -1.2758065  -0.82709366 -0.40474826]\n",
      "Iteration: 300  Norm: 4.161994934082031  theta: [-1.5439749  -0.22423369 -1.3037015  -0.9119768  -0.37245965]\n",
      "Iteration: 400  Norm: 0.4395751953125  theta: [-1.5494919  -0.23826328 -1.3090049  -0.92812485 -0.3663207 ]\n",
      "Convergence reached after 447 iterations. \n",
      "Time: 5.415534734725952 seconds. Norm: 0.0990447998046875\n"
     ]
    }
   ],
   "source": [
    "theta_MLE_homo = minimize_adam(likelihood, grad_likelihood, jnp.ones(5), lr=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula to retrieve MLE Standard Errors\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla l_{it} &= \\frac{\\partial l_{it} (\\theta)}{\\partial \\theta} \\tag{column vector} \\\\\n",
    "SE(\\widehat{\\theta}) &= diag\\Bigg[{\\sqrt{\\Big(\\sum_{i=1}^N \\sum_{t=1}^T \\nabla l_{it} \\cdot \\nabla l_{it}' \\Big)^{-1}}}\\Bigg]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Computation of standard errors\n",
    "\n",
    "@jit\n",
    "def likelihood_it(theta, i, t):\n",
    "    \"\"\"\n",
    "    Computes the likelihood for an individual observation\n",
    "    \"\"\"\n",
    "    probas_theta = choice_probas(theta)\n",
    "    likelihood_it = jnp.log(probas_theta[t, choice_jnp[i, t]])\n",
    "    return likelihood_it\n",
    "\n",
    "grad_likelihood_it = jit(grad(likelihood_it)) ### Takes the gradient of the individual likelihood\n",
    "\n",
    "@jit\n",
    "def outer_grad_likelihood(theta, i, t):\n",
    "    \"\"\"\n",
    "    Takes the outer product (column vector x row vector) of the gradient of the individual likelihood\n",
    "    \"\"\"\n",
    "    grad_it = (grad_likelihood_it(theta, i, t)).reshape(-1, 1) \n",
    "    return grad_it@grad_it.T\n",
    "\n",
    "\n",
    "#computes the outer product above for each individual and time period\n",
    "grad_likelihood_it_vec = vmap(vmap(outer_grad_likelihood, in_axes=(None, 0, None)), in_axes=(None, None, 0)) \n",
    "\n",
    "@jit\n",
    "def compute_standard_errors(theta):\n",
    "    sum_outers = (jnp.sum(grad_likelihood_it_vec(theta, jnp.arange(N), jnp.arange(T)), axis=(0, 1)))\n",
    "    return jnp.diag(jnp.sqrt(jnp.linalg.inv(sum_outers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE assuming homogeneous $\\Theta^h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\Theta^h$: [-1.5499601  -0.23945011 -1.309453   -0.92949164 -0.36580124]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$se$: [0.0242196  0.04772623 0.02251137 0.05515778 0.02070105]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "se = compute_standard_errors(theta_MLE_homo)\n",
    "display(Latex(f'$\\Theta^h$: {theta_MLE_homo}'))\n",
    "display(Latex(f'$se$: {se}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6226189 -0.3826288 -1.3769871 -1.094965  -0.4279044]\n",
      "[-1.4773014  -0.09627143 -1.2419189  -0.7640183  -0.3036981 ]\n"
     ]
    }
   ],
   "source": [
    "###Two classes: instead of estimating theta, we want to estimate the weights phi_1, phi_2 of each class (?)\n",
    "theta_k1 = theta_MLE_homo - 3*se\n",
    "theta_k2 = theta_MLE_homo + 3*se\n",
    "print(theta_k1)\n",
    "print(theta_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def choice_probas_2classes(phi1):\n",
    "    phi2 = 1 - phi1\n",
    "    probas_k1 = choice_probas(theta_k1)\n",
    "    probas_k2 = choice_probas(theta_k2)\n",
    "    probas = phi1 * probas_k1 + phi2 * probas_k2\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def likelihood_2classes(phi1): #(log)-likelihood function\n",
    "    probas_theta = choice_probas_2classes(phi1) #get the choice probabilities for the candidate theta\n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[jnp.arange(T), choice_jnp])) #sum the log-probabilities of the observed choices\n",
    "    return -log_likelihood\n",
    "\n",
    "\n",
    "grad_likelihood_2classes = jit(grad(likelihood_2classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 47 iterations. \n",
      "Time: 0.5442581176757812 seconds. Norm: 0.0\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_1^h$: [-1.6226189 -0.3826288 -1.3769871 -1.094965  -0.4279044]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\Theta_2^h$: [-1.4773014  -0.09627143 -1.2419189  -0.7640183  -0.3036981 ]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (0.5217000246047974, 0.47829997539520264)\n"
     ]
    }
   ],
   "source": [
    "weight_1 = minimize_adam(likelihood_2classes, grad_likelihood_2classes, 0.5, verbose=False)\n",
    "display(Latex(f'$\\Theta_1^h$: {theta_k1}'))\n",
    "display(Latex(f'$\\Theta_2^h$: {theta_k2}'))\n",
    "print(f'Weights: {weight_1.item(), 1-weight_1.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE assuming that $\\Theta^h$ has a normal distribution across the households \n",
    "\n",
    "Prompt, given to French AI 'Le Chat, by Mistral AI':\n",
    "* Yo le chat ! Je voudrais faire maximum likelihood, under assumption that beta is a random coefficient that is normally distributed. HOw do I do this ?\n",
    "* I'll show you my current code for MLE estimation under homogeneous theta. Show me how to modify it to get what I want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_probas_vec = vmap(choice_probas)\n",
    "\n",
    "@jit\n",
    "def likelihood_normal(params, n_draws=500): #(log)-likelihood function\n",
    "    mu, log_sigma = params[:5], params[5:]\n",
    "\n",
    "    sigma = jnp.exp(log_sigma)\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "\n",
    "    theta_list = mu + sigma * jax.random.normal(key, (n_draws, len(mu)))\n",
    "    probas_theta = choice_probas_vec(theta_list)\n",
    "    \n",
    "    log_likelihood = jnp.sum(jnp.log(probas_theta[:, jnp.arange(T), choice_jnp]))\n",
    "    return -log_likelihood/n_draws\n",
    "\n",
    "grad_likelihood_normal = jit(grad(likelihood_normal)) ## gradient of the likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_start = jnp.concatenate((theta_MLE_homo, jnp.diag(jnp.ones(5)).flatten()))\n",
    "x_start = jnp.concatenate((theta_MLE_homo, jnp.ones(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(94102.47, dtype=float32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_normal(x_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  Norm: 256.5911865234375  theta: [-1.5359179  -0.20468949 -1.2961324  -0.8844269  -0.38002786 -0.00313769\n",
      "  0.00301184 -0.00271258 -0.00261913  0.00476202]\n",
      "Iteration: 200  Norm: 2.271724224090576  theta: [-1.5472858e+00 -2.3251373e-01 -1.3069135e+00 -9.2161208e-01\n",
      " -3.6882609e-01  2.9319970e-05 -3.6787351e-05  2.6668191e-05\n",
      "  2.0714982e-05  1.9729567e-05]\n",
      "Iteration: 300  Norm: 0.15042029321193695  theta: [-1.5498588e+00 -2.3918414e-01 -1.3093596e+00 -9.2919540e-01\n",
      " -3.6591575e-01  9.2118945e-08  5.0928367e-07  7.0738949e-08\n",
      "  1.1398337e-06 -2.2799252e-07]\n",
      "Convergence reached after 324 iterations. \n",
      "Time: 48.29884672164917 seconds. Norm: 0.09304982423782349\n"
     ]
    }
   ],
   "source": [
    "theta_normal = minimize_adam(likelihood_normal, grad_likelihood_normal, x_start, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta Normal: [-1.5499992  -0.23954648 -1.3094931  -0.9296078  -0.36575806]\n",
      "Variance matrix: [0.0002194  0.00047981 0.00019497 0.00072211 0.0003106 ]\n"
     ]
    }
   ],
   "source": [
    "print(f'Theta Normal: {theta_normal[:5]}')\n",
    "print(f'Variance matrix: {jnp.sqrt(jnp.abs(theta_normal[5:]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EIO_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
